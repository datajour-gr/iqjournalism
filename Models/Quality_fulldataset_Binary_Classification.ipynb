{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Import Plotting Libararies\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Model Evaluation Libraries\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>result</th>\n",
       "      <th>tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>britney spears left court in los angeles barefoot on friday after arriving alongside her mother who is reportedly trying to obtain her daughters medical files and treatment plan from her conservator dad jamie the 37yearold singer was said to have arrived holding hands with lynne spears who wants to look at the records because her daughter is not doing well according to pictures showed the star leaving court completely barefoot in a red skirt and black top after the private hearing held behind closed doors because her medical condition was discussed a judge is understood to have ordered an expert evaluation of britney which it is thought will include a mental examination the pop sensation is also understood to have asked for more freedoms under the conservatorship but that request was not granted fans of the superstar were pictured outside court protesting to set her free and her sister jamie lynn posted an image of her sibling on instagram with a star emoji britneys father jamie has been the singers sole conservator for the past 11 years which means he is currently the only person who can look at those files the site added that lynne does not want to become her daughters coconservator but britney is said to be siding with her mother in court because she reportedly wants freedom and it has been claimed that she is hoping her mother will ease the restrictions lynne does not want to become a coconservator which creates a weird legal argument the right to medical information but no right to act on it a source told the site spears appeared in court at her own request to speak to a judge who oversees the conservatorship that has controlled her affairs for 11 years los angeles superior court judge brenda penny cleared the courtroom friday of all members of the public and media before spears appeared so only those involved in the case know what she said when the doors reopened the courtroom was empty and penny has issued no ruling or statement on spears case spears was brought in and out of the courthouse via backdoors and side entrances and only a few determined fans and passersby caught glimpses of her the singer doesnt normally attend hearings on her conservatorship which gives her father jamie spears and various lawyers power over much of her life britney and lynne were estranged for almost 15 years but they have grown closer recently under californian law a conservatorship is a court case where a judge appoints a responsible person the conservator to care for another adult the conservatee that person is said to not be able to care for himself or herself or manage his or her own finances in appointing a conservator the court is said to be guided by the best interests of the conservatee with the selection up to a judge it is usually a permanent arrangement over the last few months lynne has tried to reestablish a relationship the source added britney was granted a temporary restraining order wednesday against a former confidante who she says has been harassing her family again a judge ordered 44yearold sam lutfi who has been in legal fights with the spears family for a decade to stay at least 200 yards from her her parents and her two sons who are 12 and 13 the judge also ordered that he not contact or disparage anyone from the family the restraining order petition alleges lutfi has been sending harassing and threatening texts to spears family and disparaging them on social media lutfi was a major presence in spears life at the height of her fame and had claimed he was her manager in the years leading up to a public meltdown in 2008 her troubles led to the establishment of a courtordered conservatorship which allowed her father jamie and attorneys to run her affairs and remains in effect a decade later a judge ruled late wednesday that a lawyer for lynne could be part of fridays proceeding this week lynne has been staying at britneys thousand oaks california mansion britney cut lynne out of her life right after her breakup with justin timberlake claimed a source britney and justin split in 2002 after three years together he went on to date cameron diaz and is now married to jessica biel its been claimed that lynne was worried about britney after reading online she was forced to take certain drugs at a mental health facility for 30 days further reports say the superstarsjudgment is impaired because her medications are still not working right she has reportedly been diagnosed as bipolar and according to webmdcom finding the right balance of medications is tricky it was added the formula has to be changed every few years this is called maintenance therapy the medications usually used to treat bipolar disorder are olanzapine zyprexa quetiapine seroquel risperidone risperdal ariprazole abilify ziprasidone geodon and clozapine clozaril britney is making bad decisions and its a problem a source told her meds stopped working and doctors have been unable to find the right combination of drugs earlier this week claimed the 64yearold mississippiborn matriarch wants to step because jamie is ill but sources have said lynne is too lax the grandmotheroffive is said to be way too permissive and just wants britney to like her while her 66yearold exhusband has been trying to provide structure and discipline spears has been under courtordered permanent conservatorship of her father jamie and attorney andrew wallet since being put on 5150 involuntary psychiatric hold in 2008 lynne was last pictured on monday cradling 13monthold granddaughter ivey while on board an airplane with her 28yearold daughter jamie lynn last weekend the former mouseketeer whos lost 5lbs from stress posted snaps of her workout session as well as a mystery shoot with photographer frances iacuzzi its been four months since britney canceled her second las vegas residency domination to put my full focus and energy on my family and father jamie who almost died in november once upon a one more time a broadwaybound musical featuring music from the divorced motheroftwo is still scheduled to open november 13 at the james m nederlander theatre in chicago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>the nine mormons massacred in mexico are tied to the 1988 blood atonement killing of a texas family who tried to leave a cult associated with their community according to reports mark chynoweth and his brother duane were gunned down along with the latters eightyearold daughter jennifer in houston in june of that year after leaving polygamous mormon cult the church of the lamb of god kprc two reports it was founded by ervil lebaron who urged his followers to kill anyone who did respect my leadership he died in prison in 1981 but a hit list drawn up by the cult leader included the names of the chynoweth brothers the three mothers and six children killed in the brutal attack at the beginning of the month were dual us mexican citizens and members of the twin mormon communities of lebaron in chihuahua state and la mora in the state of sonora many residents of the two communities that lie a fivehour bonejarring drive apart are related the spreadout community traces its origins to the end of polygamy more than a century ago by the church of jesus christ of latterday saints forcing mormon families in the us with multiple wives to establish offshoots elsewhere the lebaron family was founded by alma dayer lebaron he set up the community with his family in 1924 after being excommunicated from the church alma passed the leadership over to his son joel when he died in 1951 he subsequently incorporated the community as part of salt lake city church of the firstborn his younger brother ervil was his second in charge at the time the brothers later fell out over the leadership of the community and ervil went on to set up another sect in san diego california in 1972 evril who had at least 13 wives had joel killed in 1972 he was tried and convicted in mexico for joels murder in 1974 while in prison he wrote the 400page commandment to kill disobedient church members who were included in a hit list evril died in prison in 1981 but six family members organized four oclock murders which saw an eightyearold child and three former members shot dead within minutes of each other in texas in 1988 his son heber was held in connection with slayings in texas and utah in the 1980s another sons aaron was jailed for 45 years in a conviction connected to the murders one of evrils daughter anna lebaron said my father would order mobstyle hits and those would be carried out by his cult members if they stopped believing in him or his practice or religion and left or sometimes it was rival cult leaders that were bloodatoned for being false prophets retired houston police officer richard holland said of those involved in the 1988 killings stolen cars they were heavy into credit card abuse forgery theyd also done a couple of bank robberies they were all horrible crimes they were the last in a string of many horrible crimes but the fact that an 8yearold girl was the last victim yeah that stuck former federal prosecutor terry clark added its in the covenant children who are disciplined and wont obey are to be killed people who dont respect my leadership are to be killed at least 33 people were said to have been murdered in the 1970s during the lebaron family war six cult members wanted over the 1988 murders were arrested in phoenix on auto theft charges two weeks later after cynthia lebaron came forward prosecutor clark said she was part of the group actually involved in the homicides in houston along with sister natasha lebaron what had occurred since then is that they had killed natasha lebaron in mexico so cynthia became concerned that my goodness i might be next on the list im out of here and came forward at that time all of the suspects in that case would not be prosecuted until 2011 last months attack occurred as the women traveled with their children to visit relatives some la mora residents are now wondering whether they should stay or flee the cartel presence a constant both there and around the sibling community of colonia lebaron on the other side of the sierra madre mountains mexican officials said the attack may have been a case of mistaken identity as the women and children traveled in suvs similar to those used by cartel strongmen but family members believe the caravan was intentionally targeted for reasons still unclear to them the area around lebaron is said to be under the thumb of a juarez cartelaligned gang known as la linea around la mora its the sinaloa cartel of convicted drug lord joaquín el chapo guzmán who is serving life in a colorado maximumsecurity prison as recently as 2009 two members of the chihuahua mormon community including one from the lebaron family were killed in apparent revenge after security forces tracked drug gang members julián lebarón whose brother benjamín was slain in 2009 after standing up to local thugs in lebaron said people regularly run across men with guns who stop them on the road and ask where they are going sometimes offering to help its almost like its so integrated in our community that everybody buys and sells or deals with these people on different levels lebarón said these people are murderers and if we tolerate crime in the hopes that its not going to happen to us eventually it will he said</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>around 1200 carriages one in every 11 in the fleet allowed to keep running to avoid mass new year service cancellations the deadline of 1 january 2020 to make all trains accessible for disabled people has been ripped up in what has been branded the first broken promise since boris johnsons election win around 1200 carriages many of them on notorious northern routes already dogged by delays and cancellations will continue running into the next decade can reveal the tally is one in every 11 in the fleet and 50 per cent higher than the number that rail companies had asked for permission to keep in operation despite having had a full decade to prepare some cannot accommodate wheelchairs while others lack audiovisual information systems easy to use handholds and handrails or an accessible toilet if toilets are fitted at all the move was sharply criticised by campaigners for better transport and for people with disabilities as a bitter new year pill for passengers to swallow pms new brexit bill scraps powers for mps to scrutinise trade deal tanni greythompson the multi gold medalwinning paralympian said im hugely disappointed the rail industry has to urgently address what it is going to do to allow accessible transport and be transparent with its plans it feels like there is tinkering around the edges theyve had plenty of time but just kept pushing it into the long grass darren shirley the chief executive of campaign for better transport said with 10 years to prepare for the deadline to make trains accessible this is a delay too far and has let down disabled passengers and andy mcdonald labours transport spokesman said this government is not yet a week old but is already breaking its promises they said these clappedout trains would go and disabled passengers would have better access you cant believe a word they say no fewer than 10 operating companies have been granted permission to breach the accessibility standards that were due to come into force on 1 january they include northern rail and its infamous pacer trains bus frames mounted on train wheels as a stopgap measure more than 30 years ago which david cameron vowed to banish way back in 2014 a total of 180 pacers will remain in use in a letter seen by chris heatonharris the rail minister said he had reluctantly agreed to loosen the deadline but blamed the rail companies the new plan is for the carriages to be replaced between april and next december a full years delay although it is unclear how achievable those new deadlines will be the queens speech what she said and what boris johnson meantqueens speech promises radical review of constitution and courtsdespite the tories we can still have our first climate parliamentinside politics boris johnson has the queen read yet another speech the ministers letter says despite such a significant period of time for the rail industry to prepare i understand that were all noncompliant trains removed from service there would be a disproportionately negative effect on the provision of services for passengers thus i have reluctantly agreed to issue strictly timelimited dispensation notices to a number of operators for around 1200 carriages in the national fleet mr heatonharris said the rail firms must do more to prevent the practice of operating rail replacement bus and coach services that are not compliant with relevant accessibility regulations where noncompliant buses were used they must provide passengers who require it with accessible alternative transport such as taxis the letter said northern and transport for wales tfw had already made clear they would have to continue to use pacer trains which have no dedicated spaces for passengers using wheelchairs northern blamed a delay in the arrival of new trains being built by a spanish manufacturer while tfw said trains refurbished by rolling stock company porterbrook would not be ready</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>a nun who was locked in a property battle with katy perry reportedly claims the pop superstar has blood on her hands after an elderly sister died during a court hearing rita callanan of the order of the most holy and immaculate heart of the blessed virgin mary says she believes the singer has a lot to answer for katy and 89yearold sister catherine rose holzman had locked horns over the singers plans to buy the los feliz convent in los angeles and turn it into a luxury home sister catherine and sister rita had tried to sell it to a businesswoman called dana hollister for 31000 instead of the 108m katy had offered the archdiocese of la katy and the archdiocese then successfully sued dana and were awarded 15m 12m between them as a result however sister catherine sudenly collapsed and died in court during a hearing about the matter in march 2018 i really dont like katy perry im sure she doesnt like me 81yearold sister rita has now told the page six column of the new york post she reportedly added she believes the multimillionaire fireworks singer has blood on her hands according to sister rita sister catherines last words before dying were katy perry please stop sister rita admits that perhaps the sisters acted without authority by selling the property with the all clear from their diocese we asked dana to buy our property as we didnt want it to go to katy perry yes we put the wheels in motion to sell our property she said was it legal probably not entirely but it wasnt legal for katy perry to buy it either in media interviews the nuns behind the sale made it clear they were not big fans of the charttopper famed for her flamboyant outfits and raunchy stage shows i found katy perry and i found her videos and if its all right to say i wasnt happy with any of it sister rita told the la times in 2015 after making her offer perry a pastors daughter reportedly met with the nuns showed them her tattoo of jesus on her wrist and even performed the church hymn oh happy day however despite the stars effort the matter still ended up in the courts after they ruled in her favour perrys attorney eric rowen said justice has been well served tonight the archdiocese of la stated the archdiocese has always sought to act in the best interest of all ihm sisters both to fulfil their request to sell the institutes property and to provide for their care the archdiocese considered ms perrys allcash offer to be the best offer for the institute and for the sisters it provided the ihm institute with funds sufficient for the continued care of the sisters it also assured that the property would be used as a family home instead of for commercial purposes allowing for the property to be restored and maintained with respect for its rich history ms perry has respected the process for the sale of the property and has expressed shared concern for the sisters throughout this matter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>a man who was in a coma for three years while his family fought for his life support machines to be switched off has suddenly died while they were not there kavan maddocks 23 stopped breathing unexpectedly on saturday having been kept alive by artificial feeding and breathing tubes since plunging four storeys from a car park in november 2016 his fall left him severely brain damaged his father frank maddocks had been battling for his son pictured to be allowed to die because he said he was stuck in a cabbagelike state the reported last week but on saturday mr maddocks received a phone call from the stocksbridge brain injury clinic near sheffield to say kavan had died suddenly after a chest infection he had had all week the 47yearold said they never told us he had a chest infection maybe if they had we could have been there for him when he passed he added i am torn because in some ways i am over the moon im absolutely chuffed for him that hes finally found a way out of this lifeless hell he has been in but i will always be disappointed that i had to push for three years we had hoped the doctors would see the sense and let him go with us all there at his bedside but it did not turn out like that because kavans death was unexpected the police were called and a coroners inquest will be held mr maddocks of ossett west yorkshire went to the clinic on sunday to see his sons body having received the call at 1030pm the day before just after a family barbecue with kavans 18yearold brother kavan had died peacefully with no sign of struggle he said recalling his visit mr maddocks said i put my arms around kavan and gave him a big kiss and a hug and i told him he was in a better place now i whispered to him im pleased for you mate kavan a trainee accountant from macclesfield cheshire was 20 when he fell from the multistorey car park he had been drinking after attending a macclesfield town football match kavans mother janthea died in 2017 aged 46 a clinic spokesman said it would liaise closely with relatives to make their lives easier</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   result  \\\n",
       "0  1        \n",
       "1  1        \n",
       "2  1        \n",
       "3  0        \n",
       "4  0        \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      tokenized  \n",
       "0  britney spears left court in los angeles barefoot on friday after arriving alongside her mother who is reportedly trying to obtain her daughters medical files and treatment plan from her conservator dad jamie the 37yearold singer was said to have arrived holding hands with lynne spears who wants to look at the records because her daughter is not doing well according to pictures showed the star leaving court completely barefoot in a red skirt and black top after the private hearing held behind closed doors because her medical condition was discussed a judge is understood to have ordered an expert evaluation of britney which it is thought will include a mental examination the pop sensation is also understood to have asked for more freedoms under the conservatorship but that request was not granted fans of the superstar were pictured outside court protesting to set her free and her sister jamie lynn posted an image of her sibling on instagram with a star emoji britneys father jamie has been the singers sole conservator for the past 11 years which means he is currently the only person who can look at those files the site added that lynne does not want to become her daughters coconservator but britney is said to be siding with her mother in court because she reportedly wants freedom and it has been claimed that she is hoping her mother will ease the restrictions lynne does not want to become a coconservator which creates a weird legal argument the right to medical information but no right to act on it a source told the site spears appeared in court at her own request to speak to a judge who oversees the conservatorship that has controlled her affairs for 11 years los angeles superior court judge brenda penny cleared the courtroom friday of all members of the public and media before spears appeared so only those involved in the case know what she said when the doors reopened the courtroom was empty and penny has issued no ruling or statement on spears case spears was brought in and out of the courthouse via backdoors and side entrances and only a few determined fans and passersby caught glimpses of her the singer doesnt normally attend hearings on her conservatorship which gives her father jamie spears and various lawyers power over much of her life britney and lynne were estranged for almost 15 years but they have grown closer recently under californian law a conservatorship is a court case where a judge appoints a responsible person the conservator to care for another adult the conservatee that person is said to not be able to care for himself or herself or manage his or her own finances in appointing a conservator the court is said to be guided by the best interests of the conservatee with the selection up to a judge it is usually a permanent arrangement over the last few months lynne has tried to reestablish a relationship the source added britney was granted a temporary restraining order wednesday against a former confidante who she says has been harassing her family again a judge ordered 44yearold sam lutfi who has been in legal fights with the spears family for a decade to stay at least 200 yards from her her parents and her two sons who are 12 and 13 the judge also ordered that he not contact or disparage anyone from the family the restraining order petition alleges lutfi has been sending harassing and threatening texts to spears family and disparaging them on social media lutfi was a major presence in spears life at the height of her fame and had claimed he was her manager in the years leading up to a public meltdown in 2008 her troubles led to the establishment of a courtordered conservatorship which allowed her father jamie and attorneys to run her affairs and remains in effect a decade later a judge ruled late wednesday that a lawyer for lynne could be part of fridays proceeding this week lynne has been staying at britneys thousand oaks california mansion britney cut lynne out of her life right after her breakup with justin timberlake claimed a source britney and justin split in 2002 after three years together he went on to date cameron diaz and is now married to jessica biel its been claimed that lynne was worried about britney after reading online she was forced to take certain drugs at a mental health facility for 30 days further reports say the superstarsjudgment is impaired because her medications are still not working right she has reportedly been diagnosed as bipolar and according to webmdcom finding the right balance of medications is tricky it was added the formula has to be changed every few years this is called maintenance therapy the medications usually used to treat bipolar disorder are olanzapine zyprexa quetiapine seroquel risperidone risperdal ariprazole abilify ziprasidone geodon and clozapine clozaril britney is making bad decisions and its a problem a source told her meds stopped working and doctors have been unable to find the right combination of drugs earlier this week claimed the 64yearold mississippiborn matriarch wants to step because jamie is ill but sources have said lynne is too lax the grandmotheroffive is said to be way too permissive and just wants britney to like her while her 66yearold exhusband has been trying to provide structure and discipline spears has been under courtordered permanent conservatorship of her father jamie and attorney andrew wallet since being put on 5150 involuntary psychiatric hold in 2008 lynne was last pictured on monday cradling 13monthold granddaughter ivey while on board an airplane with her 28yearold daughter jamie lynn last weekend the former mouseketeer whos lost 5lbs from stress posted snaps of her workout session as well as a mystery shoot with photographer frances iacuzzi its been four months since britney canceled her second las vegas residency domination to put my full focus and energy on my family and father jamie who almost died in november once upon a one more time a broadwaybound musical featuring music from the divorced motheroftwo is still scheduled to open november 13 at the james m nederlander theatre in chicago  \n",
       "1  the nine mormons massacred in mexico are tied to the 1988 blood atonement killing of a texas family who tried to leave a cult associated with their community according to reports mark chynoweth and his brother duane were gunned down along with the latters eightyearold daughter jennifer in houston in june of that year after leaving polygamous mormon cult the church of the lamb of god kprc two reports it was founded by ervil lebaron who urged his followers to kill anyone who did respect my leadership he died in prison in 1981 but a hit list drawn up by the cult leader included the names of the chynoweth brothers the three mothers and six children killed in the brutal attack at the beginning of the month were dual us mexican citizens and members of the twin mormon communities of lebaron in chihuahua state and la mora in the state of sonora many residents of the two communities that lie a fivehour bonejarring drive apart are related the spreadout community traces its origins to the end of polygamy more than a century ago by the church of jesus christ of latterday saints forcing mormon families in the us with multiple wives to establish offshoots elsewhere the lebaron family was founded by alma dayer lebaron he set up the community with his family in 1924 after being excommunicated from the church alma passed the leadership over to his son joel when he died in 1951 he subsequently incorporated the community as part of salt lake city church of the firstborn his younger brother ervil was his second in charge at the time the brothers later fell out over the leadership of the community and ervil went on to set up another sect in san diego california in 1972 evril who had at least 13 wives had joel killed in 1972 he was tried and convicted in mexico for joels murder in 1974 while in prison he wrote the 400page commandment to kill disobedient church members who were included in a hit list evril died in prison in 1981 but six family members organized four oclock murders which saw an eightyearold child and three former members shot dead within minutes of each other in texas in 1988 his son heber was held in connection with slayings in texas and utah in the 1980s another sons aaron was jailed for 45 years in a conviction connected to the murders one of evrils daughter anna lebaron said my father would order mobstyle hits and those would be carried out by his cult members if they stopped believing in him or his practice or religion and left or sometimes it was rival cult leaders that were bloodatoned for being false prophets retired houston police officer richard holland said of those involved in the 1988 killings stolen cars they were heavy into credit card abuse forgery theyd also done a couple of bank robberies they were all horrible crimes they were the last in a string of many horrible crimes but the fact that an 8yearold girl was the last victim yeah that stuck former federal prosecutor terry clark added its in the covenant children who are disciplined and wont obey are to be killed people who dont respect my leadership are to be killed at least 33 people were said to have been murdered in the 1970s during the lebaron family war six cult members wanted over the 1988 murders were arrested in phoenix on auto theft charges two weeks later after cynthia lebaron came forward prosecutor clark said she was part of the group actually involved in the homicides in houston along with sister natasha lebaron what had occurred since then is that they had killed natasha lebaron in mexico so cynthia became concerned that my goodness i might be next on the list im out of here and came forward at that time all of the suspects in that case would not be prosecuted until 2011 last months attack occurred as the women traveled with their children to visit relatives some la mora residents are now wondering whether they should stay or flee the cartel presence a constant both there and around the sibling community of colonia lebaron on the other side of the sierra madre mountains mexican officials said the attack may have been a case of mistaken identity as the women and children traveled in suvs similar to those used by cartel strongmen but family members believe the caravan was intentionally targeted for reasons still unclear to them the area around lebaron is said to be under the thumb of a juarez cartelaligned gang known as la linea around la mora its the sinaloa cartel of convicted drug lord joaquín el chapo guzmán who is serving life in a colorado maximumsecurity prison as recently as 2009 two members of the chihuahua mormon community including one from the lebaron family were killed in apparent revenge after security forces tracked drug gang members julián lebarón whose brother benjamín was slain in 2009 after standing up to local thugs in lebaron said people regularly run across men with guns who stop them on the road and ask where they are going sometimes offering to help its almost like its so integrated in our community that everybody buys and sells or deals with these people on different levels lebarón said these people are murderers and if we tolerate crime in the hopes that its not going to happen to us eventually it will he said                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \n",
       "2  around 1200 carriages one in every 11 in the fleet allowed to keep running to avoid mass new year service cancellations the deadline of 1 january 2020 to make all trains accessible for disabled people has been ripped up in what has been branded the first broken promise since boris johnsons election win around 1200 carriages many of them on notorious northern routes already dogged by delays and cancellations will continue running into the next decade can reveal the tally is one in every 11 in the fleet and 50 per cent higher than the number that rail companies had asked for permission to keep in operation despite having had a full decade to prepare some cannot accommodate wheelchairs while others lack audiovisual information systems easy to use handholds and handrails or an accessible toilet if toilets are fitted at all the move was sharply criticised by campaigners for better transport and for people with disabilities as a bitter new year pill for passengers to swallow pms new brexit bill scraps powers for mps to scrutinise trade deal tanni greythompson the multi gold medalwinning paralympian said im hugely disappointed the rail industry has to urgently address what it is going to do to allow accessible transport and be transparent with its plans it feels like there is tinkering around the edges theyve had plenty of time but just kept pushing it into the long grass darren shirley the chief executive of campaign for better transport said with 10 years to prepare for the deadline to make trains accessible this is a delay too far and has let down disabled passengers and andy mcdonald labours transport spokesman said this government is not yet a week old but is already breaking its promises they said these clappedout trains would go and disabled passengers would have better access you cant believe a word they say no fewer than 10 operating companies have been granted permission to breach the accessibility standards that were due to come into force on 1 january they include northern rail and its infamous pacer trains bus frames mounted on train wheels as a stopgap measure more than 30 years ago which david cameron vowed to banish way back in 2014 a total of 180 pacers will remain in use in a letter seen by chris heatonharris the rail minister said he had reluctantly agreed to loosen the deadline but blamed the rail companies the new plan is for the carriages to be replaced between april and next december a full years delay although it is unclear how achievable those new deadlines will be the queens speech what she said and what boris johnson meantqueens speech promises radical review of constitution and courtsdespite the tories we can still have our first climate parliamentinside politics boris johnson has the queen read yet another speech the ministers letter says despite such a significant period of time for the rail industry to prepare i understand that were all noncompliant trains removed from service there would be a disproportionately negative effect on the provision of services for passengers thus i have reluctantly agreed to issue strictly timelimited dispensation notices to a number of operators for around 1200 carriages in the national fleet mr heatonharris said the rail firms must do more to prevent the practice of operating rail replacement bus and coach services that are not compliant with relevant accessibility regulations where noncompliant buses were used they must provide passengers who require it with accessible alternative transport such as taxis the letter said northern and transport for wales tfw had already made clear they would have to continue to use pacer trains which have no dedicated spaces for passengers using wheelchairs northern blamed a delay in the arrival of new trains being built by a spanish manufacturer while tfw said trains refurbished by rolling stock company porterbrook would not be ready                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \n",
       "3  a nun who was locked in a property battle with katy perry reportedly claims the pop superstar has blood on her hands after an elderly sister died during a court hearing rita callanan of the order of the most holy and immaculate heart of the blessed virgin mary says she believes the singer has a lot to answer for katy and 89yearold sister catherine rose holzman had locked horns over the singers plans to buy the los feliz convent in los angeles and turn it into a luxury home sister catherine and sister rita had tried to sell it to a businesswoman called dana hollister for 31000 instead of the 108m katy had offered the archdiocese of la katy and the archdiocese then successfully sued dana and were awarded 15m 12m between them as a result however sister catherine sudenly collapsed and died in court during a hearing about the matter in march 2018 i really dont like katy perry im sure she doesnt like me 81yearold sister rita has now told the page six column of the new york post she reportedly added she believes the multimillionaire fireworks singer has blood on her hands according to sister rita sister catherines last words before dying were katy perry please stop sister rita admits that perhaps the sisters acted without authority by selling the property with the all clear from their diocese we asked dana to buy our property as we didnt want it to go to katy perry yes we put the wheels in motion to sell our property she said was it legal probably not entirely but it wasnt legal for katy perry to buy it either in media interviews the nuns behind the sale made it clear they were not big fans of the charttopper famed for her flamboyant outfits and raunchy stage shows i found katy perry and i found her videos and if its all right to say i wasnt happy with any of it sister rita told the la times in 2015 after making her offer perry a pastors daughter reportedly met with the nuns showed them her tattoo of jesus on her wrist and even performed the church hymn oh happy day however despite the stars effort the matter still ended up in the courts after they ruled in her favour perrys attorney eric rowen said justice has been well served tonight the archdiocese of la stated the archdiocese has always sought to act in the best interest of all ihm sisters both to fulfil their request to sell the institutes property and to provide for their care the archdiocese considered ms perrys allcash offer to be the best offer for the institute and for the sisters it provided the ihm institute with funds sufficient for the continued care of the sisters it also assured that the property would be used as a family home instead of for commercial purposes allowing for the property to be restored and maintained with respect for its rich history ms perry has respected the process for the sale of the property and has expressed shared concern for the sisters throughout this matter                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \n",
       "4  a man who was in a coma for three years while his family fought for his life support machines to be switched off has suddenly died while they were not there kavan maddocks 23 stopped breathing unexpectedly on saturday having been kept alive by artificial feeding and breathing tubes since plunging four storeys from a car park in november 2016 his fall left him severely brain damaged his father frank maddocks had been battling for his son pictured to be allowed to die because he said he was stuck in a cabbagelike state the reported last week but on saturday mr maddocks received a phone call from the stocksbridge brain injury clinic near sheffield to say kavan had died suddenly after a chest infection he had had all week the 47yearold said they never told us he had a chest infection maybe if they had we could have been there for him when he passed he added i am torn because in some ways i am over the moon im absolutely chuffed for him that hes finally found a way out of this lifeless hell he has been in but i will always be disappointed that i had to push for three years we had hoped the doctors would see the sense and let him go with us all there at his bedside but it did not turn out like that because kavans death was unexpected the police were called and a coroners inquest will be held mr maddocks of ossett west yorkshire went to the clinic on sunday to see his sons body having received the call at 1030pm the day before just after a family barbecue with kavans 18yearold brother kavan had died peacefully with no sign of struggle he said recalling his visit mr maddocks said i put my arms around kavan and gave him a big kiss and a hug and i told him he was in a better place now i whispered to him im pleased for you mate kavan a trainee accountant from macclesfield cheshire was 20 when he fell from the multistorey car park he had been drinking after attending a macclesfield town football match kavans mother janthea died in 2017 aged 46 a clinic spokesman said it would liaise closely with relatives to make their lives easier                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#expanding the dispay of text sms column\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "\n",
    "dataset = pd.read_csv(\"../data/full_dataset.csv\")\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['result', 'tokenized'], dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dataset[['tokenized', 'result']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1935/1935 [00:01<00:00, 1192.60it/s]\n"
     ]
    }
   ],
   "source": [
    "# Combining all the above stundents \n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "import re\n",
    "# Tutorial about Python regular expressions: https://pymotw.com/2/re/\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "import pickle\n",
    "\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import re\n",
    "\n",
    "def decontracted(phrase):\n",
    "    # specific\n",
    "    phrase = re.sub(r\"won't\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "\n",
    "    # general\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    return phrase\n",
    "\n",
    "\n",
    "stopwords= set(['br', 'the', 'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\",\\\n",
    "            \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', \\\n",
    "            'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their',\\\n",
    "            'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', \\\n",
    "            'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', \\\n",
    "            'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', \\\n",
    "            'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after',\\\n",
    "            'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further',\\\n",
    "            'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\\\n",
    "            'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', \\\n",
    "            's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', \\\n",
    "            've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn',\\\n",
    "            \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn',\\\n",
    "            \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", \\\n",
    "            'won', \"won't\", 'wouldn', \"wouldn't\"])\n",
    "\n",
    "preprocessed_reviews = []\n",
    "# tqdm is for printing the status bar\n",
    "for sentance in tqdm(dataset['tokenized'].values):\n",
    "    sentance = re.sub(r\"http\\S+\", \"\", sentance)\n",
    "    sentance = BeautifulSoup(sentance, 'lxml').get_text()\n",
    "    sentance = decontracted(sentance)\n",
    "    sentance = re.sub(\"\\S*\\d\\S*\", \"\", sentance).strip()\n",
    "    sentance = re.sub('[^A-Za-z]+', ' ', sentance)\n",
    "    sentance = re.sub(r'\\d', '', sentance)\n",
    "    # https://gist.github.com/sebleier/554280\n",
    "    sentance = ' '.join(e.lower() for e in sentance.split() if e.lower() not in stopwords and not e.isnumeric())\n",
    "    preprocessed_reviews.append(sentance.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1935,)\n",
      "(1935,)\n"
     ]
    }
   ],
   "source": [
    "# create dataset\n",
    "X = data['tokenized']\n",
    "Y = data['result']\n",
    "print(X.shape)\n",
    "print(Y.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extration Using Bag of Words Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "some feature names  ['aa', 'aaaand', 'aaj', 'aal', 'aamer', 'aap', 'aaps', 'aaron', 'aba', 'ababa']\n",
      "==================================================\n",
      "the type of count vectorizer  <class 'scipy.sparse.csr.csr_matrix'>\n",
      "the shape of out text BOW vectorizer  (1935, 44721)\n",
      "the number of unique words  44721\n"
     ]
    }
   ],
   "source": [
    "#BoW\n",
    "vectorizer = CountVectorizer() #in scikit-learn\n",
    "X_bow = vectorizer.fit(preprocessed_reviews)\n",
    "print(\"some feature names \", vectorizer.get_feature_names()[:10])\n",
    "print('='*50)\n",
    "\n",
    "final_counts = vectorizer.transform(preprocessed_reviews)\n",
    "print(\"the type of count vectorizer \",type(final_counts))\n",
    "print(\"the shape of out text BOW vectorizer \",final_counts.get_shape())\n",
    "print(\"the number of unique words \", final_counts.get_shape()[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "X_bow = vectorizer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze = vectorizer.build_analyzer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0002am',\n",
       " '0004',\n",
       " '001',\n",
       " '0024hrs',\n",
       " '0028',\n",
       " '0031hrs',\n",
       " '009',\n",
       " '01',\n",
       " '010',\n",
       " '02',\n",
       " '0207',\n",
       " '02c',\n",
       " '03',\n",
       " '04',\n",
       " '044',\n",
       " '05',\n",
       " '0561553gnlfw200505jfch7dwmp3',\n",
       " '06',\n",
       " '0619',\n",
       " '07810',\n",
       " '08',\n",
       " '0800',\n",
       " '0800555111',\n",
       " '0827',\n",
       " '0925hrs',\n",
       " '10',\n",
       " '100',\n",
       " '1000',\n",
       " '10000',\n",
       " '100000',\n",
       " '10000kg',\n",
       " '1000km',\n",
       " '1000page',\n",
       " '1001',\n",
       " '1007',\n",
       " '100acre',\n",
       " '100anight',\n",
       " '100ayear',\n",
       " '100bn',\n",
       " '100ft',\n",
       " '100g',\n",
       " '100km',\n",
       " '100m',\n",
       " '100million',\n",
       " '100ml',\n",
       " '100mph',\n",
       " '100percent',\n",
       " '100plus',\n",
       " '100th',\n",
       " '100year',\n",
       " '101',\n",
       " '10100',\n",
       " '1010wins',\n",
       " '1011',\n",
       " '101311',\n",
       " '1013ft',\n",
       " '1014',\n",
       " '1014am',\n",
       " '1015mph',\n",
       " '1015pm',\n",
       " '10160',\n",
       " '1017',\n",
       " '1017am',\n",
       " '1018',\n",
       " '101st',\n",
       " '102',\n",
       " '102000',\n",
       " '1020cm',\n",
       " '1024',\n",
       " '1025f',\n",
       " '10278',\n",
       " '103',\n",
       " '1030',\n",
       " '10300',\n",
       " '1030am',\n",
       " '1030pm',\n",
       " '104',\n",
       " '1040',\n",
       " '10400',\n",
       " '10405',\n",
       " '1040pm',\n",
       " '1045',\n",
       " '1045pm',\n",
       " '1048am',\n",
       " '1049am',\n",
       " '104th',\n",
       " '105',\n",
       " '1050',\n",
       " '10500',\n",
       " '105000',\n",
       " '1050am',\n",
       " '1051am',\n",
       " '105am',\n",
       " '105pm',\n",
       " '106',\n",
       " '106000',\n",
       " '10643',\n",
       " '106m',\n",
       " '106yearold',\n",
       " '107',\n",
       " '107000',\n",
       " '1075',\n",
       " '10787',\n",
       " '108',\n",
       " '108000',\n",
       " '1085',\n",
       " '108m',\n",
       " '109',\n",
       " '1091',\n",
       " '109mph',\n",
       " '10a',\n",
       " '10am',\n",
       " '10an',\n",
       " '10andahalf',\n",
       " '10billion',\n",
       " '10bn',\n",
       " '10c',\n",
       " '10cm',\n",
       " '10course',\n",
       " '10day',\n",
       " '10f',\n",
       " '10foot',\n",
       " '10ft',\n",
       " '10hour',\n",
       " '10in',\n",
       " '10kg',\n",
       " '10km',\n",
       " '10lbs',\n",
       " '10m',\n",
       " '10mg',\n",
       " '10million',\n",
       " '10minute',\n",
       " '10ml',\n",
       " '10month',\n",
       " '10monthold',\n",
       " '10p',\n",
       " '10page',\n",
       " '10pm',\n",
       " '10point',\n",
       " '10s',\n",
       " '10th',\n",
       " '10ton',\n",
       " '10week',\n",
       " '10year',\n",
       " '10yearold',\n",
       " '10yearolds',\n",
       " '10yearsold',\n",
       " '11',\n",
       " '110',\n",
       " '1100',\n",
       " '11000',\n",
       " '110000',\n",
       " '1107pm',\n",
       " '110k',\n",
       " '110km',\n",
       " '110lbs',\n",
       " '111',\n",
       " '1115pm',\n",
       " '1116am',\n",
       " '1118',\n",
       " '111million',\n",
       " '112000',\n",
       " '1120am',\n",
       " '112426',\n",
       " '11252',\n",
       " '1127pm',\n",
       " '1128',\n",
       " '112th',\n",
       " '112yearold',\n",
       " '113',\n",
       " '1130',\n",
       " '1130am',\n",
       " '1130pm',\n",
       " '11310',\n",
       " '1132',\n",
       " '1132pm',\n",
       " '1134pm',\n",
       " '113th',\n",
       " '114',\n",
       " '114000',\n",
       " '1140am',\n",
       " '11431',\n",
       " '1145',\n",
       " '1145pm',\n",
       " '1146am',\n",
       " '115',\n",
       " '1150',\n",
       " '1154pm',\n",
       " '1157am',\n",
       " '1159pm',\n",
       " '115pm',\n",
       " '116',\n",
       " '116000',\n",
       " '1164',\n",
       " '116m',\n",
       " '116th',\n",
       " '117',\n",
       " '1170',\n",
       " '118',\n",
       " '1180',\n",
       " '118000',\n",
       " '118th',\n",
       " '119',\n",
       " '11a',\n",
       " '11am',\n",
       " '11and12yearold',\n",
       " '11building',\n",
       " '11c',\n",
       " '11day',\n",
       " '11hour',\n",
       " '11m',\n",
       " '11member',\n",
       " '11million',\n",
       " '11minute',\n",
       " '11mlong',\n",
       " '11mm',\n",
       " '11monthold',\n",
       " '11page',\n",
       " '11pm',\n",
       " '11st',\n",
       " '11th',\n",
       " '11year',\n",
       " '11yearold',\n",
       " '12',\n",
       " '120',\n",
       " '1200',\n",
       " '12000',\n",
       " '120000',\n",
       " '120000aweek',\n",
       " '1201',\n",
       " '12019',\n",
       " '1204am',\n",
       " '1205',\n",
       " '120lbs',\n",
       " '120pound',\n",
       " '120seat',\n",
       " '121',\n",
       " '1215',\n",
       " '1215am',\n",
       " '1215pm',\n",
       " '122',\n",
       " '1220',\n",
       " '122000',\n",
       " '1220am',\n",
       " '122am',\n",
       " '123',\n",
       " '1230',\n",
       " '1230am',\n",
       " '1230pm',\n",
       " '1233',\n",
       " '123456',\n",
       " '1237',\n",
       " '12389ft',\n",
       " '124',\n",
       " '12419',\n",
       " '1241am',\n",
       " '1245',\n",
       " '125',\n",
       " '1250',\n",
       " '125000',\n",
       " '1256',\n",
       " '125m',\n",
       " '125million',\n",
       " '125pm',\n",
       " '125th',\n",
       " '126',\n",
       " '1260',\n",
       " '126000',\n",
       " '127',\n",
       " '127000',\n",
       " '128000',\n",
       " '1285',\n",
       " '129',\n",
       " '1290',\n",
       " '12andahalf',\n",
       " '12anti',\n",
       " '12billion',\n",
       " '12bn',\n",
       " '12c',\n",
       " '12cm',\n",
       " '12degree',\n",
       " '12ft',\n",
       " '12gauge',\n",
       " '12hour',\n",
       " '12inlong',\n",
       " '12lesson',\n",
       " '12m',\n",
       " '12million',\n",
       " '12month',\n",
       " '12pm',\n",
       " '12seater',\n",
       " '12strong',\n",
       " '12th',\n",
       " '12thfloor',\n",
       " '12to1',\n",
       " '12week',\n",
       " '12world',\n",
       " '12year',\n",
       " '12yearold',\n",
       " '13',\n",
       " '130',\n",
       " '1300',\n",
       " '13000',\n",
       " '130000',\n",
       " '130000ayear',\n",
       " '1307',\n",
       " '130am',\n",
       " '130million',\n",
       " '130pm',\n",
       " '1310',\n",
       " '13119',\n",
       " '13190652982',\n",
       " '132611',\n",
       " '132a',\n",
       " '133',\n",
       " '133000',\n",
       " '133bed',\n",
       " '134',\n",
       " '13450',\n",
       " '134am',\n",
       " '135',\n",
       " '13500',\n",
       " '135000',\n",
       " '13500m',\n",
       " '1353',\n",
       " '135bn',\n",
       " '136',\n",
       " '136000',\n",
       " '1366mm',\n",
       " '137',\n",
       " '13700',\n",
       " '137000',\n",
       " '137pm',\n",
       " '138',\n",
       " '138000',\n",
       " '13bn',\n",
       " '13c',\n",
       " '13deck',\n",
       " '13ft',\n",
       " '13m',\n",
       " '13million',\n",
       " '13month',\n",
       " '13monthold',\n",
       " '13months',\n",
       " '13term',\n",
       " '13th',\n",
       " '13tonne',\n",
       " '13year',\n",
       " '13yearold',\n",
       " '13yearolds',\n",
       " '14',\n",
       " '140',\n",
       " '1400',\n",
       " '14000',\n",
       " '140000',\n",
       " '14000squarefoot',\n",
       " '140am',\n",
       " '140amwhen',\n",
       " '140m',\n",
       " '141',\n",
       " '1416',\n",
       " '1420',\n",
       " '142000',\n",
       " '143',\n",
       " '143000',\n",
       " '143680',\n",
       " '143bn',\n",
       " '143pm',\n",
       " '144',\n",
       " '144200',\n",
       " '1444',\n",
       " '1445hrs',\n",
       " '144736',\n",
       " '144th',\n",
       " '145',\n",
       " '145000',\n",
       " '145am',\n",
       " '145cm',\n",
       " '1465',\n",
       " '1467',\n",
       " '1469',\n",
       " '147',\n",
       " '1471',\n",
       " '148',\n",
       " '1480',\n",
       " '148750',\n",
       " '1488',\n",
       " '148th',\n",
       " '149',\n",
       " '14acre',\n",
       " '14day',\n",
       " '14hour',\n",
       " '14m',\n",
       " '14monthold',\n",
       " '14page',\n",
       " '14th',\n",
       " '14year',\n",
       " '14yearold',\n",
       " '14yearolds',\n",
       " '15',\n",
       " '150',\n",
       " '1500',\n",
       " '15000',\n",
       " '150000',\n",
       " '15000acre',\n",
       " '1500s',\n",
       " '150foot',\n",
       " '150g',\n",
       " '150m',\n",
       " '150mg',\n",
       " '150page',\n",
       " '150pm',\n",
       " '150story',\n",
       " '150yearold',\n",
       " '151',\n",
       " '151470',\n",
       " '151700',\n",
       " '152',\n",
       " '1520',\n",
       " '153',\n",
       " '1530pm',\n",
       " '154',\n",
       " '1541',\n",
       " '1547',\n",
       " '1548',\n",
       " '154am',\n",
       " '155',\n",
       " '155000',\n",
       " '1562',\n",
       " '157',\n",
       " '15797',\n",
       " '157am',\n",
       " '158',\n",
       " '1580',\n",
       " '15800',\n",
       " '158bn',\n",
       " '158f',\n",
       " '158pm',\n",
       " '159',\n",
       " '159000',\n",
       " '1595',\n",
       " '15billion',\n",
       " '15bn',\n",
       " '15c',\n",
       " '15day',\n",
       " '15ft',\n",
       " '15hour',\n",
       " '15ish',\n",
       " '15m',\n",
       " '15million',\n",
       " '15mins',\n",
       " '15minute',\n",
       " '15mph',\n",
       " '15th',\n",
       " '15thcentury',\n",
       " '15to20',\n",
       " '15weekold',\n",
       " '15x1kd9fx7',\n",
       " '15year',\n",
       " '15yearold',\n",
       " '16',\n",
       " '160',\n",
       " '1600',\n",
       " '16000',\n",
       " '160000',\n",
       " '1600s',\n",
       " '16039',\n",
       " '160m',\n",
       " '161',\n",
       " '161000',\n",
       " '161million',\n",
       " '162000',\n",
       " '1621',\n",
       " '1622',\n",
       " '163',\n",
       " '163rd',\n",
       " '1640s',\n",
       " '1644',\n",
       " '1645',\n",
       " '1648',\n",
       " '165',\n",
       " '1652',\n",
       " '1654pm',\n",
       " '1656',\n",
       " '1658',\n",
       " '165mile',\n",
       " '165million',\n",
       " '166',\n",
       " '1664',\n",
       " '1665',\n",
       " '1666',\n",
       " '166million',\n",
       " '167',\n",
       " '168',\n",
       " '16820',\n",
       " '1699',\n",
       " '16billion',\n",
       " '16bn',\n",
       " '16c',\n",
       " '16ft',\n",
       " '16hour',\n",
       " '16m',\n",
       " '16million',\n",
       " '16month',\n",
       " '16monthold',\n",
       " '16oz',\n",
       " '16page',\n",
       " '16story',\n",
       " '16term',\n",
       " '16th',\n",
       " '16trillion',\n",
       " '16year',\n",
       " '16yearold',\n",
       " '16yearolds',\n",
       " '16yearsold',\n",
       " '17',\n",
       " '170',\n",
       " '1700',\n",
       " '17000',\n",
       " '171',\n",
       " '172',\n",
       " '172000',\n",
       " '1736',\n",
       " '173page',\n",
       " '1740pm',\n",
       " '174foot',\n",
       " '1750',\n",
       " '175000',\n",
       " '1755',\n",
       " '175th',\n",
       " '175thanniversary',\n",
       " '176',\n",
       " '176000',\n",
       " '177',\n",
       " '177000',\n",
       " '17713',\n",
       " '1775',\n",
       " '1776',\n",
       " '1778',\n",
       " '177m',\n",
       " '178',\n",
       " '1781',\n",
       " '1783',\n",
       " '1789',\n",
       " '179',\n",
       " '1792',\n",
       " '1795',\n",
       " '17970',\n",
       " '1798',\n",
       " '17apr',\n",
       " '17bn',\n",
       " '17m',\n",
       " '17million',\n",
       " '17month',\n",
       " '17monthold',\n",
       " '17th',\n",
       " '17weekold',\n",
       " '17year',\n",
       " '17yearold',\n",
       " '17yearolds',\n",
       " '17years',\n",
       " '18',\n",
       " '180',\n",
       " '1800',\n",
       " '18000',\n",
       " '18000ayear',\n",
       " '18002738255',\n",
       " '1800s',\n",
       " '1802',\n",
       " '180day',\n",
       " '180days',\n",
       " '180pound',\n",
       " '1810',\n",
       " '1814',\n",
       " '1815',\n",
       " '1819',\n",
       " '181st',\n",
       " '182',\n",
       " '18200',\n",
       " '1821',\n",
       " '1824',\n",
       " '1833',\n",
       " '1834',\n",
       " '1837',\n",
       " '1838',\n",
       " '1848',\n",
       " '1852',\n",
       " '1853',\n",
       " '186',\n",
       " '1863',\n",
       " '1865',\n",
       " '1867',\n",
       " '1868',\n",
       " '186page',\n",
       " '187',\n",
       " '1870',\n",
       " '1875',\n",
       " '1876',\n",
       " '1877',\n",
       " '1878',\n",
       " '188',\n",
       " '1880s',\n",
       " '18862',\n",
       " '1888',\n",
       " '1888crimesc',\n",
       " '189',\n",
       " '1890',\n",
       " '189000',\n",
       " '1891',\n",
       " '18948',\n",
       " '1895',\n",
       " '1896',\n",
       " '1898',\n",
       " '18bn',\n",
       " '18c',\n",
       " '18carat',\n",
       " '18foot',\n",
       " '18inch',\n",
       " '18karat',\n",
       " '18m',\n",
       " '18million',\n",
       " '18month',\n",
       " '18monthold',\n",
       " '18plus',\n",
       " '18rated',\n",
       " '18s',\n",
       " '18st',\n",
       " '18th',\n",
       " '18they',\n",
       " '18year',\n",
       " '18yearold',\n",
       " '18yearsold',\n",
       " '19',\n",
       " '190',\n",
       " '1900',\n",
       " '19000',\n",
       " '1900s',\n",
       " '1900word',\n",
       " '1901',\n",
       " '1905',\n",
       " '1907',\n",
       " '1908',\n",
       " '1909',\n",
       " '190cm',\n",
       " '191',\n",
       " '1910',\n",
       " '1912',\n",
       " '1913',\n",
       " '1915',\n",
       " '1916',\n",
       " '1917',\n",
       " '1918',\n",
       " '1919',\n",
       " '192',\n",
       " '1920',\n",
       " '192000',\n",
       " '1920s',\n",
       " '1921',\n",
       " '1922',\n",
       " '1923',\n",
       " '1924',\n",
       " '1925',\n",
       " '1927',\n",
       " '1928',\n",
       " '1929',\n",
       " '193',\n",
       " '1930',\n",
       " '1930s',\n",
       " '1932',\n",
       " '1934',\n",
       " '1935',\n",
       " '1936',\n",
       " '19361938',\n",
       " '19361939',\n",
       " '193639',\n",
       " '1937',\n",
       " '1938',\n",
       " '1939',\n",
       " '19391945',\n",
       " '1940',\n",
       " '1940hrs',\n",
       " '1940s',\n",
       " '1941',\n",
       " '1942',\n",
       " '1943',\n",
       " '1944',\n",
       " '1945',\n",
       " '19455',\n",
       " '1946',\n",
       " '1947',\n",
       " '1948',\n",
       " '194849',\n",
       " '1949',\n",
       " '195',\n",
       " '1950',\n",
       " '195053',\n",
       " '1950s',\n",
       " '1951',\n",
       " '1952',\n",
       " '1953',\n",
       " '1954',\n",
       " '1955',\n",
       " '1956',\n",
       " '1957',\n",
       " '1958',\n",
       " '1959',\n",
       " '1960',\n",
       " '1960s',\n",
       " '1961',\n",
       " '1962',\n",
       " '1963',\n",
       " '1964',\n",
       " '1965',\n",
       " '1966',\n",
       " '1967',\n",
       " '1968',\n",
       " '1969',\n",
       " '196970',\n",
       " '197',\n",
       " '1970',\n",
       " '1970s',\n",
       " '1970sera',\n",
       " '1971',\n",
       " '1972',\n",
       " '1973',\n",
       " '1974',\n",
       " '1975',\n",
       " '1976',\n",
       " '1977',\n",
       " '1978',\n",
       " '1979',\n",
       " '198',\n",
       " '1980',\n",
       " '1980s',\n",
       " '1981',\n",
       " '1982',\n",
       " '1983',\n",
       " '1984',\n",
       " '1985',\n",
       " '1986',\n",
       " '1987',\n",
       " '1988',\n",
       " '1989',\n",
       " '199',\n",
       " '1990',\n",
       " '1990creditbill',\n",
       " '1990s',\n",
       " '1991',\n",
       " '1992',\n",
       " '19920',\n",
       " '1993',\n",
       " '1994',\n",
       " '1995',\n",
       " '19952000',\n",
       " '1996',\n",
       " '1997',\n",
       " '1998',\n",
       " '1999',\n",
       " '19month',\n",
       " '19page',\n",
       " '19r',\n",
       " '19th',\n",
       " '19thcentury',\n",
       " '19yearold',\n",
       " '19yearolds',\n",
       " '1am',\n",
       " '1billion',\n",
       " '1bn',\n",
       " '1c',\n",
       " '1cm',\n",
       " '1day',\n",
       " '1ets',\n",
       " '1inch',\n",
       " '1inchwide',\n",
       " '1kg',\n",
       " '1km',\n",
       " '1m',\n",
       " '1million',\n",
       " '1percent',\n",
       " '1pm',\n",
       " '1s',\n",
       " '1st',\n",
       " '1stworldproblems',\n",
       " '1yearold',\n",
       " '1yearolds',\n",
       " '1½',\n",
       " '20',\n",
       " '200',\n",
       " '2000',\n",
       " '20000',\n",
       " '200000',\n",
       " '2000000',\n",
       " '20000worth',\n",
       " '2000person',\n",
       " '2000s',\n",
       " '2001',\n",
       " '20012011',\n",
       " '2002',\n",
       " '2003',\n",
       " '2004',\n",
       " '2005',\n",
       " '2006',\n",
       " '200608',\n",
       " '20062012',\n",
       " '2006both',\n",
       " '2007',\n",
       " '20072008',\n",
       " '2007her',\n",
       " '2008',\n",
       " '2009',\n",
       " '200910',\n",
       " '200hectare',\n",
       " '200m',\n",
       " '200million',\n",
       " '2010',\n",
       " '20102018',\n",
       " '2010s',\n",
       " '2011',\n",
       " '201112',\n",
       " '2012',\n",
       " '2013',\n",
       " '2013s',\n",
       " '2014',\n",
       " '201417',\n",
       " '201420',\n",
       " '20142016',\n",
       " '2014s',\n",
       " '2015',\n",
       " '201516',\n",
       " '2016',\n",
       " '201617',\n",
       " '2017',\n",
       " '201718',\n",
       " '2017he',\n",
       " '2017s',\n",
       " '2017this',\n",
       " '2018',\n",
       " '201819',\n",
       " '201840',\n",
       " '2019',\n",
       " '201920',\n",
       " '2019s',\n",
       " '202',\n",
       " '2020',\n",
       " '2020s',\n",
       " '2020she',\n",
       " '2021',\n",
       " '20212022',\n",
       " '2022',\n",
       " '2023',\n",
       " '202324',\n",
       " '20234',\n",
       " '2024',\n",
       " '202425',\n",
       " '2025',\n",
       " '2025cm',\n",
       " '2026',\n",
       " '2028',\n",
       " '202pm',\n",
       " '203',\n",
       " '2030',\n",
       " '2030ft',\n",
       " '2030mm',\n",
       " '2030s',\n",
       " '2033',\n",
       " '2034',\n",
       " '2035',\n",
       " '2035943544',\n",
       " '204',\n",
       " '2040',\n",
       " '2046',\n",
       " '2047',\n",
       " '205',\n",
       " '2050',\n",
       " '205295',\n",
       " '20568000',\n",
       " '205bn',\n",
       " '206',\n",
       " '2064',\n",
       " '208',\n",
       " '208000',\n",
       " '20833',\n",
       " '209000',\n",
       " '2092',\n",
       " '2094',\n",
       " '20aweek',\n",
       " '20bn',\n",
       " '20candidate',\n",
       " '20cm',\n",
       " '20cs',\n",
       " '20day',\n",
       " '20f',\n",
       " '20ft',\n",
       " '20kg',\n",
       " '20litre',\n",
       " '20minute',\n",
       " '20mm',\n",
       " '20month',\n",
       " '20monthold',\n",
       " '20mph',\n",
       " '20point',\n",
       " '20s',\n",
       " '20strong',\n",
       " '20th',\n",
       " '20weeks',\n",
       " '20year',\n",
       " '20yearold',\n",
       " '21',\n",
       " '210',\n",
       " '2100',\n",
       " '21000',\n",
       " '211',\n",
       " '212',\n",
       " '2122',\n",
       " '2123',\n",
       " '2124',\n",
       " '213',\n",
       " '214000',\n",
       " '214am',\n",
       " '215',\n",
       " '2157',\n",
       " '215am',\n",
       " '215th',\n",
       " '216',\n",
       " '217',\n",
       " '2177',\n",
       " '21778',\n",
       " '2183',\n",
       " '21883',\n",
       " '219',\n",
       " '21924',\n",
       " '21billion',\n",
       " '21bn',\n",
       " '21day',\n",
       " '21hour',\n",
       " '21m',\n",
       " '21million',\n",
       " '21st',\n",
       " '21stcentury',\n",
       " '21yearold',\n",
       " '22',\n",
       " '220',\n",
       " '2200',\n",
       " '22000',\n",
       " '220000',\n",
       " '2200kg',\n",
       " '220am',\n",
       " '221',\n",
       " '221977',\n",
       " '222',\n",
       " '223',\n",
       " '223000',\n",
       " '224',\n",
       " '225',\n",
       " '225p',\n",
       " '226',\n",
       " '227',\n",
       " '22744800',\n",
       " '2280',\n",
       " '229',\n",
       " '2297',\n",
       " '22acre',\n",
       " '22day',\n",
       " '22gz',\n",
       " '22m',\n",
       " '22month',\n",
       " '22monthlong',\n",
       " '22monthold',\n",
       " '22nd',\n",
       " '22year',\n",
       " '22yearold',\n",
       " '22yrs',\n",
       " '23',\n",
       " '230',\n",
       " '2300',\n",
       " '23000',\n",
       " '23000plus',\n",
       " '2300word',\n",
       " '230am',\n",
       " '230m',\n",
       " '230pm',\n",
       " '230vote',\n",
       " '231',\n",
       " '2326',\n",
       " '232he',\n",
       " '232pm',\n",
       " '233000',\n",
       " '234',\n",
       " '234000',\n",
       " '234pm',\n",
       " '235',\n",
       " '2352',\n",
       " '237',\n",
       " '238',\n",
       " '238160',\n",
       " '2385',\n",
       " '238pm',\n",
       " '239000',\n",
       " '23bn',\n",
       " '23day',\n",
       " ...]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1935, 47155)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_bow.toarray()\n",
    "X_bow.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction Using TfIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfTransformer(smooth_idf=False)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "transformer = TfidfTransformer(smooth_idf=False)\n",
    "transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "some sample features(unique words in the corpus) ['aaron', 'abandon', 'abandoned', 'abandoning', 'abducted', 'ability', 'ablaze', 'able', 'able get', 'aboard']\n",
      "==================================================\n",
      "the type of count vectorizer  <class 'scipy.sparse.csr.csr_matrix'>\n",
      "the shape of out text TFIDF vectorizer  (1935, 10031)\n",
      "the number of unique words including both unigrams and bigrams  10031\n"
     ]
    }
   ],
   "source": [
    "tf_idf_vec = TfidfVectorizer(ngram_range=(1,2), min_df=10)\n",
    "tf_idf_vec.fit(preprocessed_reviews)\n",
    "print(\"some sample features(unique words in the corpus)\",tf_idf_vec.get_feature_names()[0:10])\n",
    "print('='*50)\n",
    "\n",
    "tf_idf_data = tf_idf_vec.transform(preprocessed_reviews)\n",
    "print(\"the type of count vectorizer \",type(tf_idf_data))\n",
    "print(\"the shape of out text TFIDF vectorizer \",tf_idf_data.get_shape())\n",
    "print(\"the number of unique words including both unigrams and bigrams \", tf_idf_data.get_shape()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         aaron  abandon  abandoned  abandoning  abducted  ability  ablaze  \\\n",
      "0     0.000000  0.0      0.0        0.000000    0.0       0.0      0.0      \n",
      "1     0.051835  0.0      0.0        0.000000    0.0       0.0      0.0      \n",
      "2     0.000000  0.0      0.0        0.000000    0.0       0.0      0.0      \n",
      "3     0.000000  0.0      0.0        0.000000    0.0       0.0      0.0      \n",
      "4     0.000000  0.0      0.0        0.000000    0.0       0.0      0.0      \n",
      "...        ...  ...      ...             ...    ...       ...      ...      \n",
      "1930  0.000000  0.0      0.0        0.000000    0.0       0.0      0.0      \n",
      "1931  0.000000  0.0      0.0        0.000000    0.0       0.0      0.0      \n",
      "1932  0.000000  0.0      0.0        0.023401    0.0       0.0      0.0      \n",
      "1933  0.000000  0.0      0.0        0.000000    0.0       0.0      0.0      \n",
      "1934  0.000000  0.0      0.0        0.000000    0.0       0.0      0.0      \n",
      "\n",
      "          able  able get  aboard  ...  youth  youths  youtube  youve  \\\n",
      "0     0.023663  0.0       0.0     ...  0.0    0.0     0.0      0.0     \n",
      "1     0.000000  0.0       0.0     ...  0.0    0.0     0.0      0.0     \n",
      "2     0.000000  0.0       0.0     ...  0.0    0.0     0.0      0.0     \n",
      "3     0.000000  0.0       0.0     ...  0.0    0.0     0.0      0.0     \n",
      "4     0.000000  0.0       0.0     ...  0.0    0.0     0.0      0.0     \n",
      "...        ...  ...       ...     ...  ...    ...     ...      ...     \n",
      "1930  0.000000  0.0       0.0     ...  0.0    0.0     0.0      0.0     \n",
      "1931  0.000000  0.0       0.0     ...  0.0    0.0     0.0      0.0     \n",
      "1932  0.000000  0.0       0.0     ...  0.0    0.0     0.0      0.0     \n",
      "1933  0.000000  0.0       0.0     ...  0.0    0.0     0.0      0.0     \n",
      "1934  0.000000  0.0       0.0     ...  0.0    0.0     0.0      0.0     \n",
      "\n",
      "      youve got  zealand  zelensky  zero  zero tolerance  zone  \n",
      "0     0.0        0.0      0.0       0.0   0.0             0.0   \n",
      "1     0.0        0.0      0.0       0.0   0.0             0.0   \n",
      "2     0.0        0.0      0.0       0.0   0.0             0.0   \n",
      "3     0.0        0.0      0.0       0.0   0.0             0.0   \n",
      "4     0.0        0.0      0.0       0.0   0.0             0.0   \n",
      "...   ...        ...      ...       ...   ...             ...   \n",
      "1930  0.0        0.0      0.0       0.0   0.0             0.0   \n",
      "1931  0.0        0.0      0.0       0.0   0.0             0.0   \n",
      "1932  0.0        0.0      0.0       0.0   0.0             0.0   \n",
      "1933  0.0        0.0      0.0       0.0   0.0             0.0   \n",
      "1934  0.0        0.0      0.0       0.0   0.0             0.0   \n",
      "\n",
      "[1935 rows x 10031 columns]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#create dataframe\n",
    "tf_idf_dataframe=pd.DataFrame(tf_idf_data.toarray(),columns = tf_idf_vec.get_feature_names())\n",
    "print(tf_idf_dataframe)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without Smoothing:\n",
      "      0002am  0004  001  0024hrs  0028  0031hrs  009   01  010   02  ...  \\\n",
      "0     0.0     0.0   0.0  0.0      0.0   0.0      0.0  0.0  0.0  0.0  ...   \n",
      "1     0.0     0.0   0.0  0.0      0.0   0.0      0.0  0.0  0.0  0.0  ...   \n",
      "2     0.0     0.0   0.0  0.0      0.0   0.0      0.0  0.0  0.0  0.0  ...   \n",
      "3     0.0     0.0   0.0  0.0      0.0   0.0      0.0  0.0  0.0  0.0  ...   \n",
      "4     0.0     0.0   0.0  0.0      0.0   0.0      0.0  0.0  0.0  0.0  ...   \n",
      "...   ...     ...   ...  ...      ...   ...      ...  ...  ...  ...  ...   \n",
      "1930  0.0     0.0   0.0  0.0      0.0   0.0      0.0  0.0  0.0  0.0  ...   \n",
      "1931  0.0     0.0   0.0  0.0      0.0   0.0      0.0  0.0  0.0  0.0  ...   \n",
      "1932  0.0     0.0   0.0  0.0      0.0   0.0      0.0  0.0  0.0  0.0  ...   \n",
      "1933  0.0     0.0   0.0  0.0      0.0   0.0      0.0  0.0  0.0  0.0  ...   \n",
      "1934  0.0     0.0   0.0  0.0      0.0   0.0      0.0  0.0  0.0  0.0  ...   \n",
      "\n",
      "      zygmunt   zyprexa  zzzzz  zürich  árainn  édith  éric  özersay  čeferin  \\\n",
      "0     0.0      0.040766  0.0    0.0     0.0     0.0    0.0   0.0      0.0       \n",
      "1     0.0      0.000000  0.0    0.0     0.0     0.0    0.0   0.0      0.0       \n",
      "2     0.0      0.000000  0.0    0.0     0.0     0.0    0.0   0.0      0.0       \n",
      "3     0.0      0.000000  0.0    0.0     0.0     0.0    0.0   0.0      0.0       \n",
      "4     0.0      0.000000  0.0    0.0     0.0     0.0    0.0   0.0      0.0       \n",
      "...   ...           ...  ...    ...     ...     ...    ...   ...      ...       \n",
      "1930  0.0      0.000000  0.0    0.0     0.0     0.0    0.0   0.0      0.0       \n",
      "1931  0.0      0.000000  0.0    0.0     0.0     0.0    0.0   0.0      0.0       \n",
      "1932  0.0      0.000000  0.0    0.0     0.0     0.0    0.0   0.0      0.0       \n",
      "1933  0.0      0.000000  0.0    0.0     0.0     0.0    0.0   0.0      0.0       \n",
      "1934  0.0      0.000000  0.0    0.0     0.0     0.0    0.0   0.0      0.0       \n",
      "\n",
      "      čeferins  \n",
      "0     0.0       \n",
      "1     0.0       \n",
      "2     0.0       \n",
      "3     0.0       \n",
      "4     0.0       \n",
      "...   ...       \n",
      "1930  0.0       \n",
      "1931  0.0       \n",
      "1932  0.0       \n",
      "1933  0.0       \n",
      "1934  0.0       \n",
      "\n",
      "[1935 rows x 47155 columns]\n",
      "\n",
      "\n",
      "With Smoothing:\n",
      "      0002am  0004  001  0024hrs  0028  0031hrs  009   01  010   02  ...  \\\n",
      "0     0.0     0.0   0.0  0.0      0.0   0.0      0.0  0.0  0.0  0.0  ...   \n",
      "1     0.0     0.0   0.0  0.0      0.0   0.0      0.0  0.0  0.0  0.0  ...   \n",
      "2     0.0     0.0   0.0  0.0      0.0   0.0      0.0  0.0  0.0  0.0  ...   \n",
      "3     0.0     0.0   0.0  0.0      0.0   0.0      0.0  0.0  0.0  0.0  ...   \n",
      "4     0.0     0.0   0.0  0.0      0.0   0.0      0.0  0.0  0.0  0.0  ...   \n",
      "...   ...     ...   ...  ...      ...   ...      ...  ...  ...  ...  ...   \n",
      "1930  0.0     0.0   0.0  0.0      0.0   0.0      0.0  0.0  0.0  0.0  ...   \n",
      "1931  0.0     0.0   0.0  0.0      0.0   0.0      0.0  0.0  0.0  0.0  ...   \n",
      "1932  0.0     0.0   0.0  0.0      0.0   0.0      0.0  0.0  0.0  0.0  ...   \n",
      "1933  0.0     0.0   0.0  0.0      0.0   0.0      0.0  0.0  0.0  0.0  ...   \n",
      "1934  0.0     0.0   0.0  0.0      0.0   0.0      0.0  0.0  0.0  0.0  ...   \n",
      "\n",
      "      zygmunt   zyprexa  zzzzz  zürich  árainn  édith  éric  özersay  čeferin  \\\n",
      "0     0.0      0.038883  0.0    0.0     0.0     0.0    0.0   0.0      0.0       \n",
      "1     0.0      0.000000  0.0    0.0     0.0     0.0    0.0   0.0      0.0       \n",
      "2     0.0      0.000000  0.0    0.0     0.0     0.0    0.0   0.0      0.0       \n",
      "3     0.0      0.000000  0.0    0.0     0.0     0.0    0.0   0.0      0.0       \n",
      "4     0.0      0.000000  0.0    0.0     0.0     0.0    0.0   0.0      0.0       \n",
      "...   ...           ...  ...    ...     ...     ...    ...   ...      ...       \n",
      "1930  0.0      0.000000  0.0    0.0     0.0     0.0    0.0   0.0      0.0       \n",
      "1931  0.0      0.000000  0.0    0.0     0.0     0.0    0.0   0.0      0.0       \n",
      "1932  0.0      0.000000  0.0    0.0     0.0     0.0    0.0   0.0      0.0       \n",
      "1933  0.0      0.000000  0.0    0.0     0.0     0.0    0.0   0.0      0.0       \n",
      "1934  0.0      0.000000  0.0    0.0     0.0     0.0    0.0   0.0      0.0       \n",
      "\n",
      "      čeferins  \n",
      "0     0.0       \n",
      "1     0.0       \n",
      "2     0.0       \n",
      "3     0.0       \n",
      "4     0.0       \n",
      "...   ...       \n",
      "1930  0.0       \n",
      "1931  0.0       \n",
      "1932  0.0       \n",
      "1933  0.0       \n",
      "1934  0.0       \n",
      "\n",
      "[1935 rows x 47155 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#without smooth IDF\n",
    "print(\"Without Smoothing:\")\n",
    "#define tf-idf\n",
    "tf_idf_vec = TfidfVectorizer(use_idf=True, \n",
    "                        smooth_idf=False,  \n",
    "                        ngram_range=(1,1), stop_words='english') # to use only  bigrams ngram_range=(2,2)\n",
    "#transform\n",
    "tf_idf_data = tf_idf_vec.fit_transform(X)\n",
    " \n",
    "#create dataframe\n",
    "tf_idf_dataframe=pd.DataFrame(tf_idf_data.toarray(),columns = tf_idf_vec.get_feature_names())\n",
    "print(tf_idf_dataframe)\n",
    "print(\"\\n\")\n",
    " \n",
    "#with smooth\n",
    "\n",
    "tf_idf_vec_smooth = TfidfVectorizer(use_idf=True,  \n",
    "                        smooth_idf=True,  \n",
    "                        ngram_range=(1,1),stop_words='english')\n",
    "tf_idf_data_smooth = tf_idf_vec_smooth.fit_transform(X)\n",
    "print(\"With Smoothing:\")\n",
    "tf_idf_dataframe_smooth = pd.DataFrame(tf_idf_data_smooth.toarray(),columns=tf_idf_vec_smooth.get_feature_names())\n",
    "print(tf_idf_dataframe_smooth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction Using Bigrams and Trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>frequency</th>\n",
       "      <th>unigram/bigram</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>609</td>\n",
       "      <td>white house</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>570</td>\n",
       "      <td>united states</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>501</td>\n",
       "      <td>new york</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>447</td>\n",
       "      <td>mr trump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>418</td>\n",
       "      <td>prime minister</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>555949</th>\n",
       "      <td>1</td>\n",
       "      <td>0028 crews</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>555950</th>\n",
       "      <td>1</td>\n",
       "      <td>0024hrs police</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>555951</th>\n",
       "      <td>1</td>\n",
       "      <td>001 war</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>555952</th>\n",
       "      <td>1</td>\n",
       "      <td>0004 vote</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>555953</th>\n",
       "      <td>1</td>\n",
       "      <td>0002am passes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>555954 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        frequency  unigram/bigram\n",
       "0       609        white house   \n",
       "1       570        united states \n",
       "2       501        new york      \n",
       "3       447        mr trump      \n",
       "4       418        prime minister\n",
       "...     ...                   ...\n",
       "555949  1          0028 crews    \n",
       "555950  1          0024hrs police\n",
       "555951  1          001 war       \n",
       "555952  1          0004 vote     \n",
       "555953  1          0002am passes \n",
       "\n",
       "[555954 rows x 2 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_vec = CountVectorizer(stop_words='english', ngram_range=(2,2))\n",
    "# matrix of ngrams\n",
    "ngrams = c_vec.fit_transform(X)\n",
    "# count frequency of ngrams\n",
    "count_values = ngrams.toarray().sum(axis=0)\n",
    "# list of ngrams\n",
    "vocab = c_vec.vocabulary_\n",
    "X_2gram  = pd.DataFrame(sorted([(count_values[i],k) for k,i in vocab.items()], reverse=True)\n",
    "            ).rename(columns={0: 'frequency', 1:'unigram/bigram'})\n",
    "\n",
    "X_2gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>frequency</th>\n",
       "      <th>bigram/trigram</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>136</td>\n",
       "      <td>president donald trump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>82</td>\n",
       "      <td>new york city</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>58</td>\n",
       "      <td>speaker nancy pelosi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>50</td>\n",
       "      <td>special counsel robert</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>49</td>\n",
       "      <td>quid pro quo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>692327</th>\n",
       "      <td>1</td>\n",
       "      <td>0028 crews dagenham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>692328</th>\n",
       "      <td>1</td>\n",
       "      <td>0024hrs police responded</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>692329</th>\n",
       "      <td>1</td>\n",
       "      <td>001 war terror</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>692330</th>\n",
       "      <td>1</td>\n",
       "      <td>0004 vote making</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>692331</th>\n",
       "      <td>1</td>\n",
       "      <td>0002am passes breathalyser</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>692332 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        frequency              bigram/trigram\n",
       "0       136        president donald trump    \n",
       "1       82         new york city             \n",
       "2       58         speaker nancy pelosi      \n",
       "3       50         special counsel robert    \n",
       "4       49         quid pro quo              \n",
       "...     ..                  ...              \n",
       "692327  1          0028 crews dagenham       \n",
       "692328  1          0024hrs police responded  \n",
       "692329  1          001 war terror            \n",
       "692330  1          0004 vote making          \n",
       "692331  1          0002am passes breathalyser\n",
       "\n",
       "[692332 rows x 2 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_vec = CountVectorizer(stop_words='english', ngram_range=(3,3))\n",
    "# matrix of ngrams\n",
    "ngrams = c_vec.fit_transform(X)\n",
    "# count frequency of ngrams\n",
    "count_values = ngrams.toarray().sum(axis=0)\n",
    "# list of ngrams\n",
    "vocab = c_vec.vocabulary_\n",
    "X_3gram = pd.DataFrame(sorted([(count_values[i],k) for k,i in vocab.items()], reverse=True)\n",
    "            ).rename(columns={0: 'frequency', 1:'bigram/trigram'})\n",
    "\n",
    "X_3gram "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary Classification using TFIDF vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Naïve Bayes, Logistic Regression, Random Forest, SVM, Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Machine Learning Models\n",
    "from sklearn import svm  \n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train (1296,)\n",
      "Shape of X_test (639,)\n",
      "Shape of Y_train (1296,)\n",
      "Shape of Y_test (639,)\n"
     ]
    }
   ],
   "source": [
    "# split into train test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.33)\n",
    "print('Shape of X_train', X_train.shape)\n",
    "print('Shape of X_test', X_test.shape)\n",
    "print('Shape of Y_train', y_train.shape)\n",
    "print('Shape of Y_test', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_feature_num = 3468\n",
    "vectorizer = TfidfVectorizer(max_features=max_feature_num)\n",
    "train_vecs = vectorizer.fit_transform(X_train)\n",
    "test_vecs = TfidfVectorizer(max_features=max_feature_num, vocabulary=vectorizer.vocabulary_).fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NB_classifier(train_vecs, y_train, test_vecs, y_test):\n",
    "    # Training\n",
    "    GB = GaussianNB()\n",
    "    GB.fit(train_vecs, y_train)\n",
    "\n",
    "    # testing\n",
    "    test_predictionGB = GB.predict(test_vecs)\n",
    "    return classification_report(test_predictionGB, y_test) , confusion_matrix(test_predictionGB, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LR_classifier(train_vecs, y_train, test_vecs, y_test):\n",
    "    # Training\n",
    "    LR = LogisticRegression()\n",
    "    LR.fit(train_vecs, y_train)\n",
    "\n",
    "    # testing\n",
    "    test_predictionLR = LR.predict(test_vecs)\n",
    "    return classification_report(test_predictionLR, y_test) , confusion_matrix(test_predictionLR, y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RF_classifier(train_vecs, y_train, test_vecs, y_test):\n",
    "    # Training\n",
    "    RF = RandomForestClassifier(n_estimators = 450, max_depth=9, random_state=43)\n",
    "    RF.fit(train_vecs, y_train)\n",
    "\n",
    "    # Testing\n",
    "    test_predictionRF = RF.predict( test_vecs )\n",
    "    return classification_report(test_predictionRF, y_test), confusion_matrix(test_predictionRF, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SVM_classifier(train_vecs, y_train, test_vecs, y_test):\n",
    "    # Training\n",
    "    SVM = svm.LinearSVC(max_iter=100)\n",
    "    SVM.fit(train_vecs, y_train)\n",
    "\n",
    "    # Testing\n",
    "    test_predictionSVM = SVM.predict(test_vecs)\n",
    "    return classification_report(test_predictionSVM, y_test), confusion_matrix(test_predictionSVM, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Perceptron_classifier(train_vecs, y_train, test_vecs, y_test):\n",
    "\n",
    "    # Training\n",
    "    Per = Perceptron(tol=1e-3, random_state=0)\n",
    "    Per.fit(train_vecs, y_train)\n",
    "\n",
    "    # Testing\n",
    "    test_predictionPer = Per.predict(test_vecs)\n",
    "    return classification_report(test_predictionPer, y_test), confusion_matrix(test_predictionPer, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KNN_classifier(train_vecs, y_train, test_vecs, y_test):\n",
    "    # Training\n",
    "    KNN = KNeighborsClassifier()\n",
    "    KNN.fit(train_vecs, y_train)\n",
    "\n",
    "    # testing\n",
    "    test_predictionKNN = KNN.predict(test_vecs)\n",
    "    return classification_report(test_predictionKNN, y_test), confusion_matrix(test_predictionKNN, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DT_classifier(train_vecs, y_train, test_vecs, y_test):\n",
    "    # Training\n",
    "    DT = DecisionTreeClassifier()\n",
    "    DT.fit(train_vecs, y_train)\n",
    "\n",
    "    # testing\n",
    "    test_predictionDT = DT.predict(test_vecs)\n",
    "    return classification_report(test_predictionDT, y_test), confusion_matrix(test_predictionDT, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def XGB_classifier(train_vecs, y_train, test_vecs, y_test):\n",
    "    # Training\n",
    "\n",
    "    XGB = xgb.XGBClassifier(objective=\"binary:logistic\", random_state=42, eval_metric=\"auc\")\n",
    "    XGB.fit(train_vecs, y_train)\n",
    "\n",
    "    # testing\n",
    "    test_predictionXGB = XGB.predict(test_vecs)\n",
    "    return classification_report(test_predictionXGB, y_test), confusion_matrix(test_predictionXGB, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results of SVM CLASSIFIER on TF-IDF Vectorizer\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.76      0.77       313\n",
      "           1       0.77      0.78      0.78       326\n",
      "\n",
      "    accuracy                           0.77       639\n",
      "   macro avg       0.77      0.77      0.77       639\n",
      "weighted avg       0.77      0.77      0.77       639\n",
      "\n",
      "[[239  74]\n",
      " [ 72 254]]\n"
     ]
    }
   ],
   "source": [
    "class_report , conf_matrix = SVM_classifier(train_vecs, y_train, test_vecs, y_test)\n",
    "print('Results of SVM CLASSIFIER on TF-IDF Vectorizer\\n')\n",
    "print(class_report)\n",
    "print(conf_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results of Logistic Regression Classifier on TF-IDF Vectorizer\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.79      0.77       301\n",
      "           1       0.80      0.78      0.79       338\n",
      "\n",
      "    accuracy                           0.78       639\n",
      "   macro avg       0.78      0.78      0.78       639\n",
      "weighted avg       0.78      0.78      0.78       639\n",
      "\n",
      "[[237  64]\n",
      " [ 74 264]]\n"
     ]
    }
   ],
   "source": [
    "class_report , conf_matrix = LR_classifier(train_vecs, y_train, test_vecs, y_test)\n",
    "print('Results of Logistic Regression Classifier on TF-IDF Vectorizer\\n')\n",
    "print(class_report)\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results of  Random Forest on TF-IDF Vectorizer\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.75      0.78       340\n",
      "           1       0.74      0.81      0.77       299\n",
      "\n",
      "    accuracy                           0.78       639\n",
      "   macro avg       0.78      0.78      0.78       639\n",
      "weighted avg       0.78      0.78      0.78       639\n",
      "\n",
      "[[254  86]\n",
      " [ 57 242]]\n"
     ]
    }
   ],
   "source": [
    "class_report , conf_matrix = RF_classifier(train_vecs, y_train, test_vecs, y_test)\n",
    "print('Results of  Random Forest on TF-IDF Vectorizer\\n')\n",
    "print(class_report)\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results of Naive Bayes Classifier on TF-IDF Vectorizer\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.71      0.72       321\n",
      "           1       0.71      0.74      0.72       318\n",
      "\n",
      "    accuracy                           0.72       639\n",
      "   macro avg       0.72      0.72      0.72       639\n",
      "weighted avg       0.72      0.72      0.72       639\n",
      "\n",
      "[[227  94]\n",
      " [ 84 234]]\n"
     ]
    }
   ],
   "source": [
    "class_report , conf_matrix = NB_classifier(train_vecs.todense(), y_train, test_vecs.todense(), y_test)\n",
    "print('Results of Naive Bayes Classifier on TF-IDF Vectorizer\\n')\n",
    "print(class_report)\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class_report , conf_matrix = Perceptron_classifier(train_vecs, y_train, test_vecs, y_test)\n",
    "#print('Results of Perceptron Classifier on TF-IDF Vectorizer\\n')\n",
    "#print(class_report)\n",
    "#print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results of KNN_classifier on TF-IDF Vectorizer\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.49      0.65       639\n",
      "           1       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.49       639\n",
      "   macro avg       0.50      0.24      0.33       639\n",
      "weighted avg       1.00      0.49      0.65       639\n",
      "\n",
      "[[311 328]\n",
      " [  0   0]]\n"
     ]
    }
   ],
   "source": [
    "class_report , conf_matrix = KNN_classifier(train_vecs, y_train, test_vecs, y_test)\n",
    "print('Results of KNN_classifier on TF-IDF Vectorizer\\n')\n",
    "print(class_report)\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results of Decision tree classifier on TF-IDF Vectorizer\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.61      0.63       334\n",
      "           1       0.60      0.65      0.62       305\n",
      "\n",
      "    accuracy                           0.63       639\n",
      "   macro avg       0.63      0.63      0.63       639\n",
      "weighted avg       0.63      0.63      0.63       639\n",
      "\n",
      "[[203 131]\n",
      " [108 197]]\n"
     ]
    }
   ],
   "source": [
    "class_report , conf_matrix = DT_classifier(train_vecs, y_train, test_vecs, y_test)\n",
    "print('Results of Decision tree classifier on TF-IDF Vectorizer\\n')\n",
    "print(class_report)\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results of XGB classifier on TF-IDF Vectorizer\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.68      0.71       337\n",
      "           1       0.67      0.73      0.70       302\n",
      "\n",
      "    accuracy                           0.70       639\n",
      "   macro avg       0.70      0.70      0.70       639\n",
      "weighted avg       0.71      0.70      0.70       639\n",
      "\n",
      "[[229 108]\n",
      " [ 82 220]]\n"
     ]
    }
   ],
   "source": [
    "class_report , conf_matrix = XGB_classifier(train_vecs, y_train, test_vecs, y_test)\n",
    "print('Results of XGB classifier on TF-IDF Vectorizer\\n')\n",
    "print(class_report)\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary Classification using BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1296, 38811) (639, 38811)\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(binary=False,ngram_range=(1,1))\n",
    "train_vecs = vectorizer.fit_transform(X_train)\n",
    "test_vecs = vectorizer.transform(X_test)\n",
    "print (train_vecs.shape, test_vecs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: We will only consider training dataset to define the vocabulary and use the same vocabulary to \n",
    "represent the test dataset (as test data is supposed to be hidden).\n",
    "\n",
    "Thus we will fit our vectorizer on the training data and use it to transform the test data,\n",
    "\n",
    "There are 8016 unique words in vocabulary.\n",
    "\n",
    "For each review in our dataset, the Frequency of words(term-frequency) is represented through a vocabulary vector of size 8016. That’s why we have 2323 such vectors in our training-set and similarly 1145 vectors of the similar shape in our test dataset.\n",
    "\n",
    "Note: binary=False argument means that we fill the vocabulary vector with term-frequency. If binary=True, the vocabulary vector is filled by the presence of words (1 if the word is present and 0 otherwise).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results of SVM CLASSIFIER on BOW\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.72      0.76       347\n",
      "           1       0.70      0.79      0.74       292\n",
      "\n",
      "    accuracy                           0.75       639\n",
      "   macro avg       0.75      0.75      0.75       639\n",
      "weighted avg       0.76      0.75      0.75       639\n",
      "\n",
      "[[249  98]\n",
      " [ 62 230]]\n"
     ]
    }
   ],
   "source": [
    "class_report , conf_matrix = SVM_classifier(train_vecs, y_train, test_vecs, y_test)\n",
    "print('Results of SVM CLASSIFIER on BOW\\n')\n",
    "print(class_report)\n",
    "print(conf_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results of Logistic Regression Classifier on BOW \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.73      0.77       346\n",
      "           1       0.72      0.81      0.76       293\n",
      "\n",
      "    accuracy                           0.77       639\n",
      "   macro avg       0.77      0.77      0.77       639\n",
      "weighted avg       0.77      0.77      0.77       639\n",
      "\n",
      "[[254  92]\n",
      " [ 57 236]]\n"
     ]
    }
   ],
   "source": [
    "class_report , conf_matrix = LR_classifier(train_vecs, y_train, test_vecs, y_test)\n",
    "print('Results of Logistic Regression Classifier on BOW \\n')\n",
    "print(class_report)\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results of  Random Forest on BOW \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.70      0.76       366\n",
      "           1       0.67      0.80      0.73       273\n",
      "\n",
      "    accuracy                           0.74       639\n",
      "   macro avg       0.75      0.75      0.74       639\n",
      "weighted avg       0.76      0.74      0.75       639\n",
      "\n",
      "[[257 109]\n",
      " [ 54 219]]\n"
     ]
    }
   ],
   "source": [
    "class_report , conf_matrix = RF_classifier(train_vecs, y_train, test_vecs, y_test)\n",
    "print('Results of  Random Forest on BOW \\n')\n",
    "print(class_report)\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results of Naive Bayes Classifier on BOW \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.48      0.79      0.60       189\n",
      "           1       0.88      0.64      0.74       450\n",
      "\n",
      "    accuracy                           0.68       639\n",
      "   macro avg       0.68      0.71      0.67       639\n",
      "weighted avg       0.76      0.68      0.70       639\n",
      "\n",
      "[[149  40]\n",
      " [162 288]]\n"
     ]
    }
   ],
   "source": [
    "class_report , conf_matrix = NB_classifier(train_vecs.todense(), y_train, test_vecs.todense(), y_test)\n",
    "print('Results of Naive Bayes Classifier on BOW \\n')\n",
    "print(class_report)\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class_report , conf_matrix = Perceptron_classifier(train_vecs, y_train, test_vecs, y_test)\n",
    "#print('Results of Perceptron Classifier on BOW\\n')\n",
    "#print(class_report)\n",
    "#print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results of KNN_classifier on BOW\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.64      0.74       419\n",
      "           1       0.55      0.81      0.65       220\n",
      "\n",
      "    accuracy                           0.70       639\n",
      "   macro avg       0.71      0.73      0.70       639\n",
      "weighted avg       0.76      0.70      0.71       639\n",
      "\n",
      "[[270 149]\n",
      " [ 41 179]]\n"
     ]
    }
   ],
   "source": [
    "class_report , conf_matrix = KNN_classifier(train_vecs, y_train, test_vecs, y_test)\n",
    "print('Results of KNN_classifier on BOW\\n')\n",
    "print(class_report)\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results of Decision tree classifier on BOW\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.66      0.69       339\n",
      "           1       0.65      0.71      0.68       300\n",
      "\n",
      "    accuracy                           0.69       639\n",
      "   macro avg       0.69      0.69      0.69       639\n",
      "weighted avg       0.69      0.69      0.69       639\n",
      "\n",
      "[[225 114]\n",
      " [ 86 214]]\n"
     ]
    }
   ],
   "source": [
    "class_report , conf_matrix = DT_classifier(train_vecs, y_train, test_vecs, y_test)\n",
    "print('Results of Decision tree classifier on BOW\\n')\n",
    "print(class_report)\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results of XGB classifier  on BOW\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.75      0.78       333\n",
      "           1       0.75      0.80      0.78       306\n",
      "\n",
      "    accuracy                           0.78       639\n",
      "   macro avg       0.78      0.78      0.78       639\n",
      "weighted avg       0.78      0.78      0.78       639\n",
      "\n",
      "[[251  82]\n",
      " [ 60 246]]\n"
     ]
    }
   ],
   "source": [
    "class_report , conf_matrix = XGB_classifier(train_vecs, y_train, test_vecs, y_test)\n",
    "print('Results of XGB classifier  on BOW\\n')\n",
    "print(class_report)\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary Classification using Bigram/Trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1296, 394821) (639, 394821)\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(binary=False,ngram_range=(2,2))\n",
    "train_vecs = vectorizer.fit_transform(X_train)\n",
    "test_vecs = vectorizer.transform(X_test)\n",
    "print (train_vecs.shape, test_vecs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results of SVM CLASSIFIER on Bi-gram\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.73      0.78       354\n",
      "           1       0.71      0.81      0.76       285\n",
      "\n",
      "    accuracy                           0.77       639\n",
      "   macro avg       0.77      0.77      0.77       639\n",
      "weighted avg       0.78      0.77      0.77       639\n",
      "\n",
      "[[258  96]\n",
      " [ 53 232]]\n"
     ]
    }
   ],
   "source": [
    "class_report , conf_matrix = SVM_classifier(train_vecs, y_train, test_vecs, y_test)\n",
    "print('Results of SVM CLASSIFIER on Bi-gram\\n')\n",
    "print(class_report)\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results of Logistic Regression Classifier on Bi-gram\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.72      0.78       372\n",
      "           1       0.68      0.83      0.75       267\n",
      "\n",
      "    accuracy                           0.76       639\n",
      "   macro avg       0.77      0.77      0.76       639\n",
      "weighted avg       0.78      0.76      0.77       639\n",
      "\n",
      "[[266 106]\n",
      " [ 45 222]]\n"
     ]
    }
   ],
   "source": [
    "class_report , conf_matrix = LR_classifier(train_vecs, y_train, test_vecs, y_test)\n",
    "print('Results of Logistic Regression Classifier on Bi-gram\\n')\n",
    "print(class_report)\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results of  Random Forest on Bi-gram\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.68      0.76       398\n",
      "           1       0.61      0.83      0.70       241\n",
      "\n",
      "    accuracy                           0.74       639\n",
      "   macro avg       0.74      0.75      0.73       639\n",
      "weighted avg       0.77      0.74      0.74       639\n",
      "\n",
      "[[270 128]\n",
      " [ 41 200]]\n"
     ]
    }
   ],
   "source": [
    "class_report , conf_matrix = RF_classifier(train_vecs, y_train, test_vecs, y_test)\n",
    "print('Results of  Random Forest on Bi-gram\\n')\n",
    "print(class_report)\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results of Naive Bayes Classifier on Bi-gram \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.40      0.87      0.55       144\n",
      "           1       0.94      0.62      0.75       495\n",
      "\n",
      "    accuracy                           0.68       639\n",
      "   macro avg       0.67      0.75      0.65       639\n",
      "weighted avg       0.82      0.68      0.71       639\n",
      "\n",
      "[[125  19]\n",
      " [186 309]]\n"
     ]
    }
   ],
   "source": [
    "class_report , conf_matrix = NB_classifier(train_vecs.todense(), y_train, test_vecs.todense(), y_test)\n",
    "print('Results of Naive Bayes Classifier on Bi-gram \\n')\n",
    "print(class_report)\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class_report , conf_matrix = Perceptron_classifier(train_vecs, y_train, test_vecs, y_test)\n",
    "#print('Results of Perceptron Classifier on Bi-gram \\n')\n",
    "#print(class_report)\n",
    "#print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results of KNN_classifier on Bi-gram \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.49      0.66       632\n",
      "           1       0.02      0.86      0.04         7\n",
      "\n",
      "    accuracy                           0.49       639\n",
      "   macro avg       0.51      0.67      0.35       639\n",
      "weighted avg       0.99      0.49      0.65       639\n",
      "\n",
      "[[310 322]\n",
      " [  1   6]]\n"
     ]
    }
   ],
   "source": [
    "class_report , conf_matrix = KNN_classifier(train_vecs, y_train, test_vecs, y_test)\n",
    "print('Results of KNN_classifier on Bi-gram \\n')\n",
    "print(class_report)\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results of Decision tree classifier on Bii-gram \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.65      0.66       328\n",
      "           1       0.65      0.68      0.66       311\n",
      "\n",
      "    accuracy                           0.66       639\n",
      "   macro avg       0.66      0.66      0.66       639\n",
      "weighted avg       0.66      0.66      0.66       639\n",
      "\n",
      "[[212 116]\n",
      " [ 99 212]]\n"
     ]
    }
   ],
   "source": [
    "class_report , conf_matrix = DT_classifier(train_vecs, y_train, test_vecs, y_test)\n",
    "print('Results of Decision tree classifier on Bii-gram \\n')\n",
    "print(class_report)\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results of XGB classifier  on Bi-gram \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.73      0.76       333\n",
      "           1       0.73      0.78      0.75       306\n",
      "\n",
      "    accuracy                           0.76       639\n",
      "   macro avg       0.76      0.76      0.76       639\n",
      "weighted avg       0.76      0.76      0.76       639\n",
      "\n",
      "[[244  89]\n",
      " [ 67 239]]\n"
     ]
    }
   ],
   "source": [
    "class_report , conf_matrix = XGB_classifier(train_vecs, y_train, test_vecs, y_test)\n",
    "print('Results of XGB classifier  on Bi-gram \\n')\n",
    "print(class_report)\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1296, 742164) (639, 742164)\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(binary=False,ngram_range=(3,3))\n",
    "train_vecs = vectorizer.fit_transform(X_train)\n",
    "test_vecs = vectorizer.transform(X_test)\n",
    "print (train_vecs.shape, test_vecs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results of SVM CLASSIFIER on Tri-gram\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.62      0.73       447\n",
      "           1       0.48      0.82      0.60       192\n",
      "\n",
      "    accuracy                           0.68       639\n",
      "   macro avg       0.68      0.72      0.67       639\n",
      "weighted avg       0.76      0.68      0.69       639\n",
      "\n",
      "[[276 171]\n",
      " [ 35 157]]\n"
     ]
    }
   ],
   "source": [
    "class_report , conf_matrix = SVM_classifier(train_vecs, y_train, test_vecs, y_test)\n",
    "print('Results of SVM CLASSIFIER on Tri-gram\\n')\n",
    "print(class_report)\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results of Logistic Regression Classifier on Tri-gram\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.60      0.73       477\n",
      "           1       0.42      0.86      0.57       162\n",
      "\n",
      "    accuracy                           0.67       639\n",
      "   macro avg       0.67      0.73      0.65       639\n",
      "weighted avg       0.80      0.67      0.69       639\n",
      "\n",
      "[[288 189]\n",
      " [ 23 139]]\n"
     ]
    }
   ],
   "source": [
    "class_report , conf_matrix = LR_classifier(train_vecs, y_train, test_vecs, y_test)\n",
    "print('Results of Logistic Regression Classifier on Tri-gram\\n')\n",
    "print(class_report)\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results of  Random Forest on Tri-gram\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.59      0.73       506\n",
      "           1       0.37      0.90      0.52       133\n",
      "\n",
      "    accuracy                           0.65       639\n",
      "   macro avg       0.66      0.75      0.63       639\n",
      "weighted avg       0.83      0.65      0.69       639\n",
      "\n",
      "[[298 208]\n",
      " [ 13 120]]\n"
     ]
    }
   ],
   "source": [
    "class_report , conf_matrix = RF_classifier(train_vecs, y_train, test_vecs, y_test)\n",
    "print('Results of  Random Forest on Tri-gram\\n')\n",
    "print(class_report)\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results of Naive Bayes Classifier on Tri-gram \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.46      0.82      0.59       173\n",
      "           1       0.91      0.64      0.75       466\n",
      "\n",
      "    accuracy                           0.69       639\n",
      "   macro avg       0.68      0.73      0.67       639\n",
      "weighted avg       0.78      0.69      0.70       639\n",
      "\n",
      "[[142  31]\n",
      " [169 297]]\n"
     ]
    }
   ],
   "source": [
    "class_report , conf_matrix = NB_classifier(train_vecs.todense(), y_train, test_vecs.todense(), y_test)\n",
    "print('Results of Naive Bayes Classifier on Tri-gram \\n')\n",
    "print(class_report)\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class_report , conf_matrix = Perceptron_classifier(train_vecs, y_train, test_vecs, y_test)\n",
    "#print('Results of Perceptron Classifier on Tri-gram \\n')\n",
    "#print(class_report)\n",
    "#print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results of KNN_classifier on Tri-gram \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.49      0.65       639\n",
      "           1       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.49       639\n",
      "   macro avg       0.50      0.24      0.33       639\n",
      "weighted avg       1.00      0.49      0.65       639\n",
      "\n",
      "[[311 328]\n",
      " [  0   0]]\n"
     ]
    }
   ],
   "source": [
    "class_report , conf_matrix = KNN_classifier(train_vecs, y_train, test_vecs, y_test)\n",
    "print('Results of KNN_classifier on Tri-gram \\n')\n",
    "print(class_report)\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results of Decision tree classifier on Tri-gram \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.63      0.65       327\n",
      "           1       0.63      0.67      0.65       312\n",
      "\n",
      "    accuracy                           0.65       639\n",
      "   macro avg       0.65      0.65      0.65       639\n",
      "weighted avg       0.65      0.65      0.65       639\n",
      "\n",
      "[[207 120]\n",
      " [104 208]]\n"
     ]
    }
   ],
   "source": [
    "class_report , conf_matrix = DT_classifier(train_vecs, y_train, test_vecs, y_test)\n",
    "print('Results of Decision tree classifier on Tri-gram \\n')\n",
    "print(class_report)\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results of XGB classifier  on Tri-gram \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.68      0.71       337\n",
      "           1       0.67      0.73      0.70       302\n",
      "\n",
      "    accuracy                           0.70       639\n",
      "   macro avg       0.70      0.70      0.70       639\n",
      "weighted avg       0.71      0.70      0.70       639\n",
      "\n",
      "[[229 108]\n",
      " [ 82 220]]\n"
     ]
    }
   ],
   "source": [
    "class_report , conf_matrix = XGB_classifier(train_vecs, y_train, test_vecs, y_test)\n",
    "print('Results of XGB classifier  on Tri-gram \\n')\n",
    "print(class_report)\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
