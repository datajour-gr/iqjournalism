{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import requests\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "pd.options.display.max_columns\n",
    "pd.set_option(\"display.max_columns\",100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_news = pd.read_csv(\"../NEW_DATA/PENDING/nafteboriki_ellada_fb1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_news.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_news"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_letter['num_wds'] = df_letter['tokenized'].apply(lambda x: len(x.split()))\n",
    "df_news['num_wds'] = df_news['article_text'].apply(lambda x: len(x.split()) if pd.notna(x) and type(x) == str else 0)\n",
    "df_news['num_wds'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_news['num_wds'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax=df_news['num_wds'].plot(kind='hist', bins=5, fontsize=14, figsize=(6,5))\n",
    "ax.set_title('Text length\\n', fontsize=10)\n",
    "ax.set_ylabel('Frequency', fontsize=10)\n",
    "ax.set_xlabel('Number of Words', fontsize=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop under 100 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop rows with texts< 60 words\n",
    "df_news = df_news[df_news['num_wds'] >= 60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_news = df_news.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_news"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count Unique words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_news['uniq_wds'] = df_news['article_text'].apply(lambda x: len(set(x.split())) if pd.notna(x) and type(x) == str else 0)\n",
    "df_news['uniq_wds'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax=df_news['uniq_wds'].plot(kind='hist', bins=50, fontsize=14, figsize=(12,10))\n",
    "ax.set_title('Unique Words Per Text\\n', fontsize=20)\n",
    "ax.set_ylabel('Frequency', fontsize=18)\n",
    "ax.set_xlabel('Number of Unique Words', fontsize=18);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lexical Richness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def calculate_mtld(text, chunk_size=100):\n",
    "    tokens = text.split()  # Tokenize the text (simple splitting by whitespace)\n",
    "\n",
    "    unique_words = set()  # Track unique words\n",
    "    mtld_count = 0\n",
    "    total_tokens = len(tokens)\n",
    "\n",
    "    for i, token in enumerate(tokens):\n",
    "        unique_words.add(token)\n",
    "\n",
    "        if len(unique_words) >= chunk_size:\n",
    "            mtld_count += 1\n",
    "            unique_words.clear()\n",
    "\n",
    "    # Calculate MTLD score\n",
    "    mtld_score = total_tokens / (mtld_count + 1)\n",
    "\n",
    "    return mtld_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_news['MTLD'] = df_news['article_text'].apply(lambda x: calculate_mtld(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COUNT NOUNS VERBS ADJECTIVES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "# from spacy import displacy \n",
    "from spacy.matcher import Matcher\n",
    "import pandas as pd\n",
    "import unicodedata\n",
    "from unidecode import unidecode\n",
    "from Levenshtein import *\n",
    "from collections import defaultdict\n",
    "import textdescriptives as td  #TEXTDESCRIPTIVES FR PSACY3\n",
    "import numpy as np\n",
    "nlp = spacy.load(\"el_core_news_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process text and count nouns, verbs, and adjectives\n",
    "def find_nouns_verbs_adjectives(text):\n",
    "    doc = nlp(text)\n",
    "    nouns = sum(1 for token in doc if token.pos_ == \"NOUN\")\n",
    "    verbs = sum(1 for token in doc if token.pos_ == \"VERB\")\n",
    "    adjectives = sum(1 for token in doc if token.pos_ == \"ADJ\")\n",
    "    return nouns, verbs, adjectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to each row of the DataFrame\n",
    "df_news[[\"Nouns\", \"Verbs\", \"Adjectives\"]] = df_news[\"article_text\"].apply(lambda x: pd.Series(find_nouns_verbs_adjectives(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_news[\"Nouns\"] = df_news[\"Nouns\"]/ df_news['num_wds'] *100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_news[\"Verbs\"] = df_news[\"Verbs\"]/ df_news['num_wds'] *100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_news[\"Adjectives\"] = df_news[\"Adjectives\"]/ df_news['num_wds'] *100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_news.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intensity Lexcicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AIL_df = pd.read_csv(\"../LEXIKA/AIL_KS.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.sentiment\n",
    "w2affect = []\n",
    "for w,score,dim in AIL_df.values:\n",
    "    w2affect.append({dim:score,'w':w})\n",
    "w2affects = {}\n",
    "df = pd.DataFrame(w2affect)\n",
    "df = df.groupby('w').sum().reset_index()\n",
    "for w,anger,fear,joy,sadness, trust, disgust, anticipation, surprise in df[['w','anger','fear','joy','sadness', 'trust', 'disgust', 'anticipation', 'surprise']].values:\n",
    "    w2affects[w] = {'anger':anger,'joy':joy,'sadness':sadness,'fear':fear,'trust': trust, 'disgust':disgust, 'anticipation':anticipation,'surprise': surprise }\n",
    "\n",
    "def get_affect_intensity_score(doc,tokenizer=nltk.word_tokenize,agg='mean'):\n",
    "    if type(doc)==str:\n",
    "        doc = tokenizer(doc)\n",
    "    assert type(doc)==list,\"please input either a list or a string\"\n",
    "    matches = []\n",
    "    for w in doc:\n",
    "        w = w.lower()\n",
    "        if w in w2affects:\n",
    "            matches.append(w2affects[w])\n",
    "    if len(matches)==0:\n",
    "        return {'anger':np.nan,'joy':np.nan,'sadness':np.nan,'fear':np.nan,'trust':np.nan,'disgust':np.nan,'anticipation':np.nan,'surprise':np.nan}\n",
    "    scores = pd.DataFrame(matches)\n",
    "    if agg=='mean':\n",
    "        scores = scores.mean()\n",
    "    elif agg=='max':\n",
    "        scores =  scores.max()\n",
    "    else:\n",
    "        scores = agg(scores)\n",
    "    return dict(scores)\n",
    "\n",
    "get_affect_intensity_score('Υπάρχει πολύ μίσος και αηδία στην ατμόσφαιρα. Είσαι απαίσιος!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only anger \n",
    "\n",
    "w2affect = []\n",
    "for w,score,dim in AIL_df.values:\n",
    "    w2affect.append({dim:score,'w':w})\n",
    "w2affects = {}\n",
    "df = pd.DataFrame(w2affect)\n",
    "df = df.groupby('w').sum().reset_index()\n",
    "for w,anger,fear,joy,sadness in df[['w','anger','fear','joy','sadness']].values:\n",
    "    w2affects[w] = {'anger':anger,'joy':joy,'sadness':sadness,'fear':fear}\n",
    "\n",
    "def get_anger_score(doc,tokenizer=nltk.word_tokenize,agg='mean'):\n",
    "    if type(doc)==str:\n",
    "        doc = tokenizer(doc)\n",
    "    assert type(doc)==list,\"please input either a list or a string\"\n",
    "    matches = []\n",
    "    for w in doc:\n",
    "        w = w.lower()\n",
    "        if w in w2affects:\n",
    "            matches.append(w2affects[w])\n",
    "    if len(matches)==0:\n",
    "        return {'anger':np.nan,'joy':np.nan,'sadness':np.nan,'fear':np.nan}\n",
    "    scores = pd.DataFrame(matches)\n",
    "    if agg=='mean':\n",
    "        scores = scores.mean()\n",
    "    elif agg=='max':\n",
    "        scores =  scores.max()\n",
    "    else:\n",
    "        scores = agg(scores)\n",
    "    return list(dict(scores).values())[0]\n",
    "\n",
    "get_anger_score('Είμαι πολύ θυμωμένος και έχω νεύρα και θυμό και δεν αντέχω!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_news ['Anger_intensity'] = df_news['cleaned_text_lemmas'].apply(lambda x: get_anger_score(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only joy\n",
    "\n",
    "w2affect = []\n",
    "for w,score,dim in AIL_df.values:\n",
    "    w2affect.append({dim:score,'w':w})\n",
    "w2affects = {}\n",
    "df = pd.DataFrame(w2affect)\n",
    "df = df.groupby('w').sum().reset_index()\n",
    "for w,anger,fear,joy,sadness in df[['w','anger','fear','joy','sadness']].values:\n",
    "    w2affects[w] = {'anger':anger,'joy':joy,'sadness':sadness,'fear':fear}\n",
    "\n",
    "def get_joy_score(doc,tokenizer=nltk.word_tokenize,agg='mean'):\n",
    "    if type(doc)==str:\n",
    "        doc = tokenizer(doc)\n",
    "    assert type(doc)==list,\"please input either a list or a string\"\n",
    "    matches = []\n",
    "    for w in doc:\n",
    "        w = w.lower()\n",
    "        if w in w2affects:\n",
    "            matches.append(w2affects[w])\n",
    "    if len(matches)==0:\n",
    "        return {'anger':np.nan,'joy':np.nan,'sadness':np.nan,'fear':np.nan}\n",
    "    scores = pd.DataFrame(matches)\n",
    "    if agg=='mean':\n",
    "        scores = scores.mean()\n",
    "    elif agg=='max':\n",
    "        scores =  scores.max()\n",
    "    else:\n",
    "        scores = agg(scores)\n",
    "    return list(dict(scores).values())[1]\n",
    "\n",
    "get_joy_score('είσαι ωραίος και χαίρομαι να σε βλέπω.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_news ['Joy_intensity'] = df_news['cleaned_text_lemmas'].apply(lambda x: get_joy_score(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only sadness\n",
    "\n",
    "w2affect = []\n",
    "for w,score,dim in AIL_df.values:\n",
    "    w2affect.append({dim:score,'w':w})\n",
    "w2affects = {}\n",
    "df = pd.DataFrame(w2affect)\n",
    "df = df.groupby('w').sum().reset_index()\n",
    "for w,anger,fear,joy,sadness in df[['w','anger','fear','joy','sadness']].values:\n",
    "    w2affects[w] = {'anger':anger,'joy':joy,'sadness':sadness,'fear':fear}\n",
    "\n",
    "def get_sadness_score(doc,tokenizer=nltk.word_tokenize,agg='mean'):\n",
    "    if type(doc)==str:\n",
    "        doc = tokenizer(doc)\n",
    "    assert type(doc)==list,\"please input either a list or a string\"\n",
    "    matches = []\n",
    "    for w in doc:\n",
    "        w = w.lower()\n",
    "        if w in w2affects:\n",
    "            matches.append(w2affects[w])\n",
    "    if len(matches)==0:\n",
    "        return {'anger':np.nan,'joy':np.nan,'sadness':np.nan,'fear':np.nan}\n",
    "    scores = pd.DataFrame(matches)\n",
    "    if agg=='mean':\n",
    "        scores = scores.mean()\n",
    "    elif agg=='max':\n",
    "        scores =  scores.max()\n",
    "    else:\n",
    "        scores = agg(scores)\n",
    "    return list(dict(scores).values())[2]\n",
    "\n",
    "get_sadness_score('Δεν μπορεί να ζούμε αυτή την τραγωδία που έπληξε τη χώρα. ')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_news ['Sadness_intensity'] = df_news['cleaned_text_lemmas'].apply(lambda x: get_sadness_score(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only fear\n",
    "\n",
    "w2affect = []\n",
    "for w,score,dim in AIL_df.values:\n",
    "    w2affect.append({dim:score,'w':w})\n",
    "w2affects = {}\n",
    "df = pd.DataFrame(w2affect)\n",
    "df = df.groupby('w').sum().reset_index()\n",
    "for w,anger,fear,joy,sadness in df[['w','anger','fear','joy','sadness']].values:\n",
    "    w2affects[w] = {'anger':anger,'joy':joy,'sadness':sadness,'fear':fear}\n",
    "\n",
    "def get_fear_score(doc,tokenizer=nltk.word_tokenize,agg='mean'):\n",
    "    if type(doc)==str:\n",
    "        doc = tokenizer(doc)\n",
    "    assert type(doc)==list,\"please input either a list or a string\"\n",
    "    matches = []\n",
    "    for w in doc:\n",
    "        w = w.lower()\n",
    "        if w in w2affects:\n",
    "            matches.append(w2affects[w])\n",
    "    if len(matches)==0:\n",
    "        return {'anger':np.nan,'joy':np.nan,'sadness':np.nan,'fear':np.nan}\n",
    "    scores = pd.DataFrame(matches)\n",
    "    if agg=='mean':\n",
    "        scores = scores.mean()\n",
    "    elif agg=='max':\n",
    "        scores =  scores.max()\n",
    "    else:\n",
    "        scores = agg(scores)\n",
    "    return list(dict(scores).values())[3]\n",
    "\n",
    "get_fear_score('Είναι φοβερή η τραγωδία που έπληξε τη χώρα. ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_news ['Fear_intensity'] = df_news['cleaned_text_lemmas'].apply(lambda x: get_fear_score(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_news [['Anger_intensity','Joy_intensity', 'Sadness_intensity','Fear_intensity']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['Anger_intensity', 'Joy_intensity', 'Sadness_intensity',\n",
    "       'Fear_intensity']\n",
    "df_news[cols] = df_news[cols].apply(pd.to_numeric, errors='coerce', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_news.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_news['joy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_news['joy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_news = df_news.rename(columns={' anticipation': 'anticipation','num_wds': 'length'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_news['disgust'] =  df_news['disgust']/ df_news['uniq_wds'] *100\n",
    "df_news['fear'] =  df_news['fear']/ df_news['uniq_wds'] *100\n",
    "df_news['surprise'] =  df_news['surprise']/ df_news['uniq_wds'] *100\n",
    "df_news['trust'] =  df_news['trust']/ df_news['uniq_wds'] *100\n",
    "df_news['sadness'] =  df_news['sadness']/ df_news['uniq_wds'] *100\n",
    "df_news['negative'] =  df_news['negative']/ df_news['uniq_wds'] *100\n",
    "df_news['joy'] =  df_news['joy']/ df_news['uniq_wds'] *100\n",
    "df_news['positive'] =  df_news['positive']/ df_news['uniq_wds'] *100\n",
    "df_news['anticipation'] =  df_news['anticipation']/ df_news['uniq_wds'] *100\n",
    "df_news['anger'] =  df_news['anger']/ df_news['uniq_wds'] *100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_news.tail(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_news['strongsubj'] =  df_news['strongsubj']/ df_news['uniq_wds'] *100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_news['weaksubj'] =  df_news['weaksubj']/ df_news['uniq_wds'] *100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count celebrities etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ιδανικά ένα if ent.label_ == \"PERSON\" ώστε να κραταει μονο τα persons όχι και τα ονόματα οδών \n",
    "\n",
    "#def get_entities(doc):\n",
    "#    entity_info = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "#    return entity_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import eli5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entities(text):\n",
    "    doc = nlp(text)\n",
    "    entity_info = [(ent.text) for ent in doc.ents if (ent.label_ == 'PERSON')]\n",
    "    return entity_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"Ήρθε η Αννα Βισση και ο Σκοιχάς και ο βατραχος και η μηχανη και η Microsoft και η Ελλάδα\")\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_entities(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_news['names'] = df_news['article_text'].apply(lambda x: get_entities(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_news['names'] = df_news['names'].apply(', '.join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_duplicates(row):\n",
    "    # Split string by ', ', drop duplicates and join back.\n",
    "    words = row.split(', ')\n",
    "    return ', '.join(np.unique(words).tolist())\n",
    "\n",
    "# drop_duplicates is applied to all rows of df.\n",
    "df_news['names'] = df_news['names'].apply(drop_duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_news['names']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "politicians = pd.read_csv('../TO_KEEP/politiciansel.csv')\n",
    "politicians.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "politicians_list = politicians['Politician'].tolist()\n",
    "politicians_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "celebrities = pd.read_csv('../TO_KEEP/celebrities')\n",
    "celebrities.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[celebrities['celebrities'].unique() for col_name in celebrities.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "celebrities_list = celebrities['celebrities'].tolist()\n",
    "celebrities_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joinedlist = celebrities_list + politicians_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_celebs(row):\n",
    "    celebrities = []\n",
    "    # Split string by ', ', drop duplicates and join back.\n",
    "    words = row.split(', ')\n",
    "    for i in words:\n",
    "        if i in joinedlist:\n",
    "            \n",
    "            celebrities.append(i)\n",
    "    return celebrities\n",
    "                \n",
    "\n",
    "\n",
    "# drop_duplicates is applied to all rows of df.\n",
    "df_news['celebrities'] = df_news['names'].apply(find_celebs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_news['celebrities']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_celebs(row):\n",
    "    celebrities = []\n",
    "    count  =0\n",
    "    # Split string by ', ', drop duplicates and join back.\n",
    "    words = row.split(', ')\n",
    "    for i in words:\n",
    "        \n",
    "        if i in joinedlist:\n",
    "            \n",
    "            celebrities.append(i)\n",
    "            count +=1 \n",
    "    return count\n",
    "                \n",
    "\n",
    "\n",
    "# drop_duplicates is applied to all rows of df.\n",
    "df_news['No Celebs'] = df_news['names'].apply(count_celebs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_news['No Celebs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_news['No Celebs'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "animals = pd.read_csv('../TO_KEEP/animals')\n",
    "animals.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "animal_list = animals['ζώο'].tolist()\n",
    "animal_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_animals(row):\n",
    "    animals = []\n",
    "    words = row.split(' ')\n",
    "    for i in words:\n",
    "        if i in animal_list:\n",
    "            animals.append(i)\n",
    "    return animals\n",
    "                \n",
    "df_news['animal'] = df_news['article_text'].apply(find_animals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_news['animal'].value_counts().head(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_animals(row):\n",
    "    animals = []\n",
    "    count  =0\n",
    "    words = row.split(' ')\n",
    "    for i in words:\n",
    "        \n",
    "        if i in animal_list:\n",
    "            \n",
    "            animals.append(i)\n",
    "            count +=1 \n",
    "    return count\n",
    "\n",
    "df_news['animal'] = df_news['article_text'].apply(count_animals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crime = pd.read_csv('../TO_KEEP/crime.csv')\n",
    "crime.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crime['crime'] = crime['crime'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crime_list = crime['crime'].tolist()\n",
    "crime_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_crime(row):\n",
    "    crime = []\n",
    "    # Split string by ', ', drop duplicates and join back.\n",
    "    words = row.split(' ')\n",
    "    #print(words)\n",
    "    for i in words:\n",
    "        if i in crime_list:\n",
    "            \n",
    "            crime.append(i)\n",
    "    return crime\n",
    "                \n",
    "\n",
    "\n",
    "# drop_duplicates is applied to all rows of df.\n",
    "df_news['crime'] = df_news['article_text'].apply(find_crime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_news['crime'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_news['crime']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_crime(row):\n",
    "    crime = []\n",
    "    count  =0\n",
    "    words = row.split(' ')\n",
    "    for i in words:\n",
    "        \n",
    "        if i in crime_list:\n",
    "            \n",
    "            crime.append(i)\n",
    "            count +=1 \n",
    "    return count\n",
    "df_news['crime'] = df_news['article_text'].apply(count_crime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_news['crime']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensual = pd.read_csv('../TO_KEEP/sensual.csv')\n",
    "sensual.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensual_list = sensual['sensual'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_sensual(row):\n",
    "    sensual = []\n",
    "    # Split string by ', ', drop duplicates and join back.\n",
    "    words = row.split(' ')\n",
    "    #print(words)\n",
    "    for i in words:\n",
    "        if i in sensual_list:\n",
    "            \n",
    "            sensual.append(i)\n",
    "    return sensual\n",
    "                \n",
    "\n",
    "\n",
    "# drop_duplicates is applied to all rows of df.\n",
    "df_news['sensual'] = df_news['article_text'].apply(find_sensual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_sensual(row):\n",
    "    sensual = []\n",
    "    count  =0\n",
    "    # Split string by ', ', drop duplicates and join back.\n",
    "    words = row.split(' ')\n",
    "    for i in words:\n",
    "        \n",
    "        if i in sensual_list:\n",
    "            \n",
    "            sensual.append(i)\n",
    "            count +=1 \n",
    "    return count\n",
    "                \n",
    "\n",
    "\n",
    "# drop_duplicates is applied to all rows of df.\n",
    "df_news['sensual'] = df_news['article_text'].apply(count_sensual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_news['sensual'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_news"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Κεφαλαία στο κείμενο"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "regex = r'\\b[A-Z]{5,}\\b'\n",
    "\n",
    "df_news['caps_body'] = df_news['article_text'].str.findall(regex)\n",
    "df_news['caps_body'] =df_news['caps_body'].str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_news['caps_body'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Νούμερα "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_news['numbers'] = df_news['article_text'].apply(lambda x: len([x for x in x.split() if x.isdigit()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_news['numbers']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_news['article_text'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ίσως να μετρήσω προσωπικές αντωνυμίες"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def find_links(text):\n",
    "    urls = re.findall('https?://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+', text)\n",
    "    return urls\n",
    "\n",
    "def number_links(text):\n",
    "    urls = re.findall('https?://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+', text)\n",
    "    number = len(urls)\n",
    "    return number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_news['urls'] = df_news.article_text.apply(find_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_news['urls'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_news['urls'] = df_news.article_text.apply(number_links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # Special Character Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_freq(x):\n",
    "    freq = 0\n",
    "    for c in ['\"', '!', '?',':','«', '»', ';']:\n",
    "       freq += x.count(c)\n",
    "    return freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_news ['special_char_ratio'] = (df_news.article_text.apply(check_freq) / df_news['uniq_wds']) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_news['special_char_title'] = (df_news.article_title.apply(check_freq) / df_news['uniq_wds']) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define subjectivity\n",
    "def define_subjectivity(column):\n",
    "    words = column.split()\n",
    "\n",
    "    strongsubj = 0\n",
    "    weaksubj = 0\n",
    "    matched_tokens = 0\n",
    "\n",
    "    for word in words:\n",
    "        found_exact = False\n",
    "        if word in indexes:\n",
    "            found_exact = True\n",
    "            indx = indexes[word]\n",
    "\n",
    "            if df.at[indx, \"Strength\"] == \"strongsubj\":\n",
    "                strongsubj = strongsubj + 1\n",
    "            elif df.at[indx, \"Strength\"] == \"weaksubj\":\n",
    "                weaksubj = weaksubj + 1\n",
    "\n",
    "            matched_tokens+= 1\n",
    "    try:\n",
    "        return (strongsubj, weaksubj)\n",
    "\n",
    "    except:\n",
    "        return False\n",
    "#Load Subjectivity vocabulary\n",
    "df = pd.read_csv(\"../LEXIKA/Greek_Subjectivity_Fixed_2023_02.csv\")\n",
    "indexes = {}\n",
    "df = df.fillna(\"N/A\")\n",
    "for index, row in df.iterrows():\n",
    "    df.at[index, \"Greek\"] = row[\"Greek\"]\n",
    "    indexes[df.at[index, \"Greek\"]] = index\n",
    "#calculate Subjectivity for lemmas\n",
    "df_news[['strongsubjTitle','weaksubjTitle']] =  df_news['article_title'].apply(lambda x: pd.Series(define_subjectivity(x)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_news['strongsubjTitle'] =  df_news['strongsubjTitle']/ df_news['uniq_wds'] *100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_news['WeakSTitle'] =  df_news['weaksubjTitle']/ df_news['uniq_wds'] *100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_news.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_news.to_csv(\"../NEW_DATA/PENDING/nafteboriki_ellada_fb1.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
